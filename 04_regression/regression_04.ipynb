{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors in Variables and Regression Dilution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that one of the assumptions of Ordinary Least Squares is that the error is entirely in the dependent $Y$ variable.  Least squares fitting of the optimal solution and the estimation of the coefficients therefore operates by minimizing the distance between the model fit with respect to the dependent variable (y) axis.\n",
    "\n",
    "However, it is often true in the real world that both $X$ and $Y$ are measured or known only with some uncertainty.  When this is the case, OLS can perform poorly and give potentially misleading results.  Here we're going to look at a particular case of regression dilution.\n",
    "\n",
    "Let's start by repeating our exercise from earlier notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# you can omit the line below if you'd like, but I really don't like the default fonts in Python, so I switch to Helvetica\n",
    "plt.rcParams['font.family'] = 'Helvetica'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, generate the same synthetic data\n",
    "np.random.seed(1999)  # seed for class reproducibility\n",
    "n_samples = 128\n",
    "\n",
    "# Create the independent variable X and the noise\n",
    "noise_magnitude = 0.5\n",
    "X = 2* np.random.rand(n_samples, 1)\n",
    "noise = noise_magnitude * np.random.randn(n_samples, 1)\n",
    "\n",
    "# Set the intercept and slope\n",
    "intercept = 4\n",
    "slope = 3\n",
    "\n",
    "# Create the dependent variable y\n",
    "y = intercept + (slope * X) + noise\n",
    "\n",
    "# create a 'noise free' line for plotting in this exercise, too\n",
    "y_known = intercept + (slope * X)\n",
    "\n",
    "# add constant term column to X\n",
    "X_with_constant = sm.add_constant(X)\n",
    "\n",
    "# Fit the model using statsmodels OLS\n",
    "model = sm.OLS(y, X_with_constant)\n",
    "\n",
    "# fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Predict y using our model - be sure to pass .predict the matrix with the constant term column!\n",
    "y_pred = results.predict(X_with_constant)\n",
    "\n",
    "\n",
    "##  Create the diagnostic plots\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# First plot: Observed vs Predicted data with regression line\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X, y, color='blue', label='Observed Data')\n",
    "plt.plot(X, y_pred, color='red', label='Predicted Data')\n",
    "plt.plot(X, y_known, color='black', linestyle='--', label='Known Relationship')\n",
    "\n",
    "plt.title('X vs y with Regression Line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "# Second plot: Residuals vs X\n",
    "residuals = y.ravel() - y_pred # note that we need to flatten or `ravel` y here because statsmodels makes y_pred a singleton variable (128,), whereas y is (128,1) !\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X, residuals, color='lightcoral')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.title('X vs Residuals')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Residuals')\n",
    "\n",
    "# Third plot: Lagged residual autocorrelation\n",
    "residuals_shifted = np.roll(residuals, 1)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(residuals[1:], residuals_shifted[1:], color='dodgerblue')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.title('Lag-1 Residual Autocorrelation')\n",
    "plt.ylabel('Residual(t-1)')\n",
    "plt.xlabel('Residual(t)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-generate our synthetic series but change the magnitude of the noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the independent variable X and the noise\n",
    "noise_magnitude = 4\n",
    "X = 2* np.random.rand(n_samples, 1)\n",
    "noise = noise_magnitude * np.random.randn(n_samples, 1)\n",
    "\n",
    "# Set the intercept and slope\n",
    "intercept = 4\n",
    "slope = 3\n",
    "\n",
    "# Create the dependent variable y\n",
    "y = intercept + (slope * X) + noise\n",
    "\n",
    "# create a 'noise free' line for plotting in this exercise, too\n",
    "y_known = intercept + (slope * X)\n",
    "\n",
    "# add constant term column to X\n",
    "X_with_constant = sm.add_constant(X)\n",
    "\n",
    "# Fit the model using statsmodels OLS\n",
    "model = sm.OLS(y, X_with_constant)\n",
    "\n",
    "# fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Predict y using our model - be sure to pass .predict the matrix with the constant term column!\n",
    "y_pred = results.predict(X_with_constant)\n",
    "\n",
    "\n",
    "##  Create the diagnostic plots\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# First plot: Observed vs Predicted data with regression line\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X, y, color='blue', label='Observed (Very Noisy) Data')\n",
    "plt.plot(X, y_pred, color='red', label='Predicted Data')\n",
    "plt.plot(X, y_known, color='black', linestyle='--', label='Known Relationship')\n",
    "\n",
    "plt.title('X vs y with Regression Line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "# Second plot: Residuals vs X\n",
    "residuals = y.ravel() - y_pred # note that we need to flatten or `ravel` y here because statsmodels makes y_pred a singleton variable (128,), whereas y is (128,1) !\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X, residuals, color='lightcoral')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.title('X vs Residuals')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Residuals')\n",
    "\n",
    "# Third plot: Lagged residual autocorrelation\n",
    "residuals_shifted = np.roll(residuals, 1)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(residuals[1:], residuals_shifted[1:], color='dodgerblue')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.title('Lag-1 Residual Autocorrelation')\n",
    "plt.ylabel('Residual(t-1)')\n",
    "plt.xlabel('Residual(t)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that despite the much larger noise magnitude, the estimated regression line still tracks the known linear relationship!  OLS is robust to noise in the $y$ variable, assuming the other assumptions about the nature of the noise are still (mostly) true.\n",
    "\n",
    "**Try changing the noise magnitude above and see what happens**\n",
    "\n",
    "But what happens if we add noise to $y$ and try and estimate the coefficients? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the independent variable X and the noise\n",
    "noise_magnitude = 0.5\n",
    "noise_magnitude_x = 2\n",
    "\n",
    "true_X = 2 * np.random.rand(n_samples, 1) # the 'real' X without noise\n",
    "\n",
    "noise = noise_magnitude * np.random.randn(n_samples, 1) # noise to add to Y, essentially\n",
    "x_noise = noise_magnitude_x * np.random.randn(n_samples, 1) # noise to add to X\n",
    "\n",
    "# Set the intercept and slope\n",
    "intercept = 4 # real intercept\n",
    "slope = 3 # real slope\n",
    "\n",
    "# create a 'noise free' line for plotting in this exercise, too\n",
    "y_known = intercept + (slope * true_X) # the real underlying relationship with intercept 4 and slope 3\n",
    "\n",
    "noise_X = true_X + x_noise # a noisy version of X where X is not observed without error\n",
    "\n",
    "# Create the dependent variable y\n",
    "y = intercept + (slope * true_X) + noise # a noisy version of y \n",
    "\n",
    "# add constant term column to the noisy version of X\n",
    "X_with_constant = sm.add_constant(noise_X)\n",
    "\n",
    "# Fit the model using statsmodels OLS\n",
    "model = sm.OLS(y, X_with_constant)\n",
    "\n",
    "# fit the model, noise in both y and X\n",
    "results = model.fit()\n",
    "\n",
    "# Predict y using our model - be sure to pass .predict the matrix with the constant term column!\n",
    "y_pred = results.predict(X_with_constant)\n",
    "\n",
    "##  Create the diagnostic plots\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# First plot: Observed vs Predicted data with regression line\n",
    "plt.subplot(1, 3, 1)\n",
    "# plt.scatter(true_X, y, color='blue', label='True Data')\n",
    "plt.scatter(noise_X, y, color='lightblue', label='Noisy Data')\n",
    "plt.plot(noise_X, y_pred, color='red', label='OLS Relationship')\n",
    "plt.plot(true_X, y_known, color='black', linestyle='--', label='Known Relationship')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "# Second plot: Residuals vs X\n",
    "residuals = y.ravel() - y_pred # note that we need to flatten or `ravel` y here because statsmodels makes y_pred a singleton variable (128,), whereas y is (128,1) !\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(true_X, residuals, color='lightcoral')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.title('X vs Residuals')\n",
    "plt.xlabel('True X')\n",
    "plt.ylabel('Residuals')\n",
    "\n",
    "# Third plot: Lagged residual autocorrelation\n",
    "residuals_shifted = np.roll(residuals, 1)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(residuals[1:], residuals_shifted[1:], color='dodgerblue')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.title('Lag-1 Residual Autocorrelation')\n",
    "plt.ylabel('Residual(t-1)')\n",
    "plt.xlabel('Residual(t)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding error to the X (independent variable) results in an attenuation of the slope -- enough unaccounted for noise in X will lead to a flat slope and an underestimation of the true magnitude of the relationship between X and Y!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An errors-in-variables approach\n",
    "\n",
    "Errors-in-variables regression methods (of which there are many) either implicitly or quantitatively allow for error or uncertainty in both the dependent and independent variables.  A few common approaches to error-in-variables regression include [Total Least Squares](https://en.wikipedia.org/wiki/Total_least_squares) and [Deming Regression](https://en.wikipedia.org/wiki/Deming_regression).  [York regression](https://pubs.aip.org/aapt/ajp/article/72/3/367/1042020/Unified-equations-for-the-slope-intercept-and) extends Deming regression by allowing correlated errors. A number of [bivariate ](https://en.wikipedia.org/wiki/Bivariate_analysis) and [orthogonal regresssion](https://docs.scipy.org/doc/scipy/reference/odr.html) methods allow for errors in both independent and dependent variables to be accounted for when finding optimal regression models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I'll just demonstrate a simple implementation of the Deming regression method.  Deming regression makes some assumptions about the errors in $X$ and $Y$: that they are independent and normally distributed and the ratio of the error variances for $X$ and $Y$ are known.  Depending on the circumstances, this latter in particular might not be true, so the code below uses a default ratio of 1 (e.g. the error magnitudes are equal for $X$ and $Y$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deming(x, y, delta=1):\n",
    "    \"\"\"Fit a Deming regression model and return slope and intercept\n",
    "\n",
    "    Inputs\n",
    "    ----------\n",
    "    x : array-like\n",
    "    y : array-like, same shape as x\n",
    "    delta: Ratio of the error variances if known\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    slope : float, Slope of the fitted line.\n",
    "    intercept : float, Intercept of the fitted line.\n",
    "    \"\"\"\n",
    "    \n",
    "    # make sure everything is an array\n",
    "    x = np.asanyarray(x)\n",
    "    y = np.asanyarray(y)\n",
    "    \n",
    "    # make sure dimensions are correct for linear algebra below\n",
    "    x = np.ravel(x)\n",
    "    y = np.ravel(y)\n",
    "    \n",
    "    # get the length of the series   \n",
    "    n = np.size(x)    \n",
    "    \n",
    "    # series means    \n",
    "    mx = x.mean()\n",
    "    my = y.mean()\n",
    "    \n",
    "    # series variances accounting for error variance ratio (default ratio is 1)\n",
    "    ds = y.var(ddof=1) - delta * x.var(ddof=1)\n",
    "    \n",
    "    # covariance of the centered data series \n",
    "    sxy = np.dot(x - mx, y - my) / (n - 1)\n",
    "    \n",
    "    # calculate slope as a function of variances and covariances\n",
    "    slope = (ds + np.sqrt(ds**2 + 4 * sxy**2)) / (2 * sxy)\n",
    "    \n",
    "    # intercept based then on slope\n",
    "    intercept = my - slope * mx\n",
    "    \n",
    "    return slope, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the independent variable X and the noise - we'll set the magnitude of the error to be the same for now - but what happens if you change this ratio and don't account for that? \n",
    "noise_magnitude = 0.5 \n",
    "noise_magnitude_x = 2\n",
    "\n",
    "true_X = 2 * np.random.rand(n_samples, 1) # the 'real' X without noise\n",
    "\n",
    "noise = noise_magnitude * np.random.randn(n_samples, 1) # noise to add to Y, essentially\n",
    "x_noise = noise_magnitude_x * np.random.randn(n_samples, 1) # noise to add to X\n",
    "\n",
    "# Set the intercept and slope\n",
    "intercept = 4 # real intercept\n",
    "slope = 3 # real slope\n",
    "\n",
    "# create a 'noise free' line for plotting in this exercise, too\n",
    "y_known = intercept + (slope * true_X) # the real underlying relationship with intercept 4 and slope 3\n",
    "\n",
    "noise_X = true_X + x_noise # a noisy version of X\n",
    "\n",
    "# Create the dependent variable y\n",
    "y = intercept + (slope * true_X) + noise # a noisy version of y accounting for noisy X?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "deming_slope, deming_intercept = deming(noise_X.flatten(), y,delta=0.5/2)\n",
    "y_pred = deming_slope * noise_X + deming_intercept\n",
    "\n",
    "# add constant term column to X\n",
    "X_with_constant = sm.add_constant(noise_X)\n",
    "model = sm.OLS(y, X_with_constant)\n",
    "results = model.fit()\n",
    "y_pred_ols = results.predict(X_with_constant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original data points and the fitted line\n",
    "plt.scatter(true_X, y, color='lightblue', label='Original data')  # Scatter plot of original data\n",
    "plt.scatter(noise_X, y, color='blue', label='Noisy X data')  # Scatter plot of original data\n",
    "\n",
    "plt.plot(noise_X, y_pred, color='red', label='Deming')  # Line of best fit\n",
    "plt.plot(noise_X, y_pred_ols, color='purple', label='OLS')\n",
    "plt.plot(true_X, y_known, color='black', label='Truth')  # Line of best fit\n",
    "\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('X values')\n",
    "plt.ylabel('Y values')\n",
    "plt.title('Original Data vs Fitted Line')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
