{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `scikit-learn` vs. `statsmodels`\n",
    "\n",
    "Both scikit-learn -- which we used last class -- and [statsmodels](https://www.statsmodels.org/stable/index.html) can do regression modeling.  Today we'll use statsmodels for our analysis.  Whereas scikit-learn is a powerful library for machine learning in general, statsmodels is a bit more focused on statistical analyses of the type you are likely to encounter for your own data.  Both libraries have their strengths, and both will work well for a variety of applications.\n",
    "\n",
    "If necessary, you can use `pip` to install statsmodels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out or remove code block if you don't need this\n",
    "# !pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get Numpy and Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statsmodels is typically imported as `sm` - if this is the very first time you've imported it or if you've just installed it, this next import step could take several seconds (don't worry). One thing to notice is that the import command isn't simply for 'statsmodels', but rather for 'statsmodels.api'.  This is different than we've done things for other libraries, and there is a description of why we might import the library this way [here](https://www.statsmodels.org/stable/api-structure.html#importpaths).  Note that you could also import directly, see [here](https://www.statsmodels.org/stable/api-structure.html#direct-import-for-programs).\n",
    "\n",
    "Let's get statsmodels now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to repeat the first example from Monday's lesson.  Let's create a simple synthetic set of $X$ and $Y$ data that are related to one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, generate the same synthetic data\n",
    "np.random.seed(1999)  # seed for class reproducibility\n",
    "n_samples = 128\n",
    "\n",
    "# Create the independent variable X and the noise\n",
    "noise_magnitude = 0.5\n",
    "X = 2* np.random.rand(n_samples, 1)\n",
    "noise = noise_magnitude * np.random.randn(n_samples, 1)\n",
    "\n",
    "# Set the intercept and slope\n",
    "intercept = 4\n",
    "slope = 3\n",
    "\n",
    "# Create the dependent variable y\n",
    "y = intercept + (slope * X) + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statsmodels uses a different syntax than scikit-learn.  First, if we want a constant term (intercept) in our regression model, we'll need to add one to our predictor variabile, $X$.  Fortunately, statsmodels anticipates this, and comes with a built-in function to do this, [.add_constant](https://www.statsmodels.org/stable/generated/statsmodels.tools.tools.add_constant.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use statsmodels to add a constant term to our predictor\n",
    "X_with_constant = sm.add_constant(X)\n",
    "print(X_with_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the left-most column in our predictor matrix is a column of ones.  This is what allows the model to find the intercept (constant) term in our regression equation. Without this, statsmodels would assume our regression has an intercept of zero.\n",
    "\n",
    "We can now fit our model.  Note that, unlike scikit-learn, the regression model is first called directly like a function (e.g we don't 'declare' the model first, we go right to passing the variables to the code).  [Here](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html) is the documentation for the OLS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using statsmodels OLS\n",
    "model = sm.OLS(y, X_with_constant)\n",
    "type(model) # model is a statsmodel linear model OLS object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `model` is an object now, we see the results by operating on `model` as an object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`results` is now an object too, and we can see the output from the regression model by using the `.summary` operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because statsmodels is built for regression modeling (amongst other tasks), it provides rich output from the model fit results.  For instance, you can see the $R^2$ value and the adjusted $R^2$.  You also get other diagnostic statistics like the Akaike Information Criterion (AIC, https://en.wikipedia.org/wiki/Akaike_information_criterion), Bayesian information criterion (BIC, https://en.wikipedia.org/wiki/Bayesian_information_criterion), the probability (based on the F-statistic), and the Log-likelihood (a goodness-of-fit measure where a regression model with a higher log likelihood indicates a better regression model than one with a lower log likelihood value).  \n",
    "\n",
    "You can also see the coefficients of your model - the constant (intercept) term is estimated as 4.0323 (very close to the known value of 4) with a standard error of 0.092 and a very high probability of being significant.  The 95% range is also shown (3.851 and 4.214), indicating that the coefficient very likely does NOT include zero.  For 'x1' (the coefficient associate with the variable $X$), we see the estimate is 2.9607 (again, very close to the known value of 3), highly significant, and again with a 95% confidence interval that does not span zero. \n",
    "\n",
    "[This Medium post](https://medium.com/swlh/interpreting-linear-regression-through-statsmodels-summary-4796d359035a) is a reasonable good review of the outcome of the OLS regression in statsmodels.\n",
    "\n",
    "We can then use our model to predict y:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict y using our model - be sure to pass .predict the matrix with the constant term column!\n",
    "y_pred = results.predict(X_with_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To mimic the diagnostic statistics from scikit-learn, we can use the following code.  For $R^2$ and the MSE we can simply extract the value from the results summary. We then calculate RMSE by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the R-squared and MSE from the model and then calculate RMSE from MSE\n",
    "r_squared = results.rsquared # just pull the statistic you want from \n",
    "mse = results.mse_resid\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Display those statistics\n",
    "print(f\"R2: {r_squared:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see all the properties of `results` (e.g other statistics from the regression model fit), you can go [here](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.RegressionResults.html).  Alternatively, if you type `dir(results)` this will list all the properties (and methods) associated with the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make out plot now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the diagnostic plots\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# First plot: Observed vs Predicted data with regression line\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X, y, color='blue', label='Observed Data')\n",
    "plt.plot(X, y_pred, color='red', label='Predicted Data')\n",
    "plt.title('X vs y with Regression Line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "# Second plot: Residuals vs X\n",
    "residuals = y.ravel() - y_pred # note that we need to flatten or `ravel` y here because statsmodels makes y_pred a singleton variable (128,), whereas y is (128,1) !\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X, residuals, color='lightcoral')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.title('X vs Residuals')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Residuals')\n",
    "\n",
    "# Third plot: Lagged residual autocorrelation\n",
    "residuals_shifted = np.roll(residuals, 1)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(residuals[1:], residuals_shifted[1:], color='dodgerblue')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.title('Lag-1 Residual Autocorrelation')\n",
    "plt.ylabel('Residual(t-1)')\n",
    "plt.xlabel('Residual(t)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
