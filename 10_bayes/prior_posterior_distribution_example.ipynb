{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian approach to multiple lines of evidence\n",
    "\n",
    "This notebook shows a short demonstration of how we can take a Bayesian approach to combining a prior with multiple independent sources of information (likelihoods, in Bayesian language) to calculate a posterior reflecting the combination of those different lines of evidence.  This demonstrations specifically shows how we can combine different conjugate distributions relatively simply.  It isn't really necessary to take a 'full Bayesian' approach here as it turns out.  But nevertheless it shows how if our prior and likelihoods are conjugate distributions, the maths behind their combination is simple. \n",
    "\n",
    "The idea and data for this example come from the following paper:\n",
    "\n",
    "> Annan, J.D. and Hargreaves, J.C., 2006. Using multiple observationally‐based constraints to estimate climate sensitivity. Geophysical Research Letters, 33(6).\n",
    "\n",
    "Annan and Hargreaves were pushing back againsts some very high values of Equilibrium Climate Sensitivity (ECS) that had been published in the literature in the prior years (e.g. see [here](https://journals.ametsoc.org/view/journals/clim/15/22/1520-0442_2002_015_3117_aobeot_2.0.co_2.xml)), many of these estimates based on perturbed physics ensembles of models.  Annan and Hargreaves argue that a more realistic prior and the combination of various independent lines of evidence (likelihoods) results in a narrower range of ECS than had been published and that their approach all but eliminates the higher tail of ECS that has appeared in the literature.  \n",
    "\n",
    "Here, we reproduce their approach, primarily to show how different distributions (here defined as a prior and several likelihoods) can be combined to give a posterior. \n",
    "\n",
    "Let's get out libraries first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gamma, norm # these are the two distributions we'll use here\n",
    "plt.rcParams['font.family'] = 'Helvetica'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the prior. Annan and Hargreaves use a notation in the paper with three number to indicate the central value and 95% limits of a probability density function describing the data:\n",
    "\n",
    "> in this notation, used throughout this paper, the central value indicates the maximum likelihood estimate in degrees Celsius and the outer values represent the limits of the 95% confidence interval for a pdf, or 95% of the area under the curve for a likelihood function.\n",
    "\n",
    "For the prior then, they therefore write: \n",
    "\n",
    "> We use as a typical representative of this class of constraints a probabilistic estimate of (1, 3, 10).  Since this distribution is strongly asymmetric, we use the gamma distribution as a parsimonious representation, using shape and scale parameters 3.2 and 1.36. We take this distribution as our prior with which additional information in the form of likelihood functions will be combined.\n",
    "\n",
    "In Python terms, we can write this using the `gamma.pdf()` with the two parameters they describe. We'll also plot this to look at it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the initial prior based on 20th-century warming (which is a gamma distribution)\n",
    "prior_shape = 3.2\n",
    "prior_scale = 1.36\n",
    "ecs_values = np.linspace(0, 10, 1000)\n",
    "prior_pdf = gamma.pdf(ecs_values, a=prior_shape, scale=prior_scale)\n",
    "\n",
    "# plot the prior distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(ecs_values, prior_pdf,'r')\n",
    "plt.title('Prior Distribution of Equilibrium Climate Sensitivity')\n",
    "plt.xlabel('ECS')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is subjective (a major complaint about Bayesian statistics) - there were a number of ways one could characterize the prior.  It is here we first encounter the potential for subjectivity in the prior and the need to select a reasonable one -- but also to be aware of its "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the likelihoods in the same way.  The authors give 2 primary constraints and 2 additional ones.  For volcanic cooling, they write this about constructing the likelihood distribution:\n",
    "\n",
    "> The short-term large-scale cooling following volcanic eruptions has also recently been used to estimate climate sensitivity ... We therefore use a gamma function with shape and scale parameters 8.5 and 0.40 as our likelihood function. The shape of this function is described by (1.5, 3, 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood for volcanic cooling (Gamma)\n",
    "volcanic_shape = 8.5\n",
    "volcanic_scale = 0.40\n",
    "volcanic_pdf = gamma.pdf(ecs_values, a=volcanic_shape, scale=volcanic_scale)\n",
    "\n",
    "# plot the likelihood distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(ecs_values, volcanic_pdf,'r')\n",
    "plt.xlabel('ECS')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They also use the Last Glacial Maximum (LGM) cooling as a constraint, write:\n",
    "\n",
    "> Temperatures at the Last Glacial Maximum (LGM) were substantially lower than the modern pre-industrial state for an extended period ...  our likelihood function has the form (−0.6, 2.7, 6.1). This near-symmetric shape is well described by the Gaussian distribution with mean 2.7°C and standard deviation 1.7°C.\n",
    "\n",
    "Note there are [more recent estimates of LGM cooling and the inferred ECS](https://www.nature.com/articles/s41586-020-2617-x) we could draw on if the paper were published today.  Expressing their likelihood in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood for LGM cooling (Normal)\n",
    "lgm_mean = 2.7\n",
    "lgm_std_dev = 1.7\n",
    "lgm_pdf = norm.pdf(ecs_values, loc=lgm_mean, scale=lgm_std_dev)\n",
    "\n",
    "# plot the likelihood distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(ecs_values, lgm_pdf,'r')\n",
    "plt.xlabel('ECS')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors also give two additional constraints.  One from model ensembles and one from cooling during the Maunder Minimum (which I think is pretty sketchy, personnally):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likelihood for ensemble models \n",
    "ensemble_mean = 3.0\n",
    "ensemble_std_dev = 1.0\n",
    "ensemble_pdf = norm.pdf(ecs_values, loc=ensemble_mean, scale=ensemble_std_dev)\n",
    "\n",
    "# likelihood for Maunder Minimum \n",
    "maunder_mean = 3.0\n",
    "maunder_std_dev = 0.675\n",
    "maunder_pdf = norm.pdf(ecs_values, loc=maunder_mean, scale=maunder_std_dev)\n",
    "\n",
    "# plot the likelihood distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(ecs_values, ensemble_pdf,label='Model Ensemble Likelihood')\n",
    "plt.plot(ecs_values, maunder_pdf,'r',label='Maunder Minimum Likelihood')\n",
    "plt.xlabel('ECS')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our prior and our various likelihoods.  The authors describe their Bayesian-like procedure thuly:\n",
    "\n",
    "> Bayes' Theorem tells us how to update a probabilistic estimate for an unknown variable x (such as climate sensitivity) in the light of new information ... We note that in the case where the new observation is conditionally independent of previous data for a given climate sensitivity, the likelihood of [the probability] of x given O. So we can iteratively combine new information with a prior probabilistic estimate simply by multiplying the prior pdf with the likelihood function arising from the new data, and renormalising appropriately.\n",
    "\n",
    "This is actually quite a simple operation.  Let's take our prior and the first likelihood, which we will multiply.  We will then 'renormali(se) appropriately' by dividing the numerator of Bayes Rule by the area under the curve of the numerator: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the prior with the volcanic cooling likelihood by multiplying the PDFs\n",
    "posterior_1 = prior_pdf * volcanic_pdf\n",
    "posterior_1 /= np.trapezoid(posterior_1, ecs_values) # normalize to area under the curve = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the description in Annan and Hargreaves (which is woefully inadequate for a paper such as this, in my opinion!), the authors appear to then proceed sequentially, using the new posterior and then updating it with new likelihoods and renormalizing at each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update posterior with LGM cooling likelihood\n",
    "posterior_2 = posterior_1 * lgm_pdf\n",
    "posterior_2 /= np.trapezoid(posterior_2, ecs_values)\n",
    "\n",
    "# update posterior with ensemble models likelihood\n",
    "posterior_3 = posterior_2 * ensemble_pdf\n",
    "posterior_3 /= np.trapezoid(posterior_3, ecs_values)\n",
    "\n",
    "# update posterior with Maunder Minimum likelihood\n",
    "final_posterior = posterior_3 * maunder_pdf\n",
    "final_posterior /= np.trapezoid(final_posterior, ecs_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the prior, the (renormalized) likelihoods, and final posterior on a single plot showing ECS distributions from each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the prior, each of the intermediate steps toward the posterior, and the final posterior\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ecs_values, prior_pdf, label='Prior (20th Century Warming)', linestyle='--', color='blue')\n",
    "plt.plot(ecs_values, volcanic_pdf, label='Volcanic Cooling Likelihood', linestyle=':', color='blue')\n",
    "plt.plot(ecs_values, lgm_pdf, label='LGM Cooling Likelihood', linestyle='-.', color='blue')\n",
    "plt.plot(ecs_values, ensemble_pdf, label='Ensemble Models Likelihood', linestyle='--', color='green')\n",
    "plt.plot(ecs_values, maunder_pdf, label='Maunder Minimum Likelihood', linestyle='-.', color='green')\n",
    "plt.plot(ecs_values, final_posterior, label='Final Posterior', linestyle='-', color='red')\n",
    "\n",
    "plt.xlabel('ECS (°C)')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean of the PDF\n",
    "mean = np.sum(ecs_values * final_posterior) * np.diff(ecs_values).mean()\n",
    "\n",
    "# calculate the median of the PDF\n",
    "cumulative = np.cumsum(final_posterior) * np.diff(ecs_values).mean()\n",
    "median = ecs_values[np.searchsorted(cumulative, 0.5)]\n",
    "\n",
    "# calculate the 2.5% and 97.5%  (e.g. the 95% range) credible intervals\n",
    "lower_25 = ecs_values[np.searchsorted(cumulative, 0.025)]\n",
    "upper_975 = ecs_values[np.searchsorted(cumulative, 0.975)]\n",
    "\n",
    "# display the results for the ECS estimate and uncertainty\n",
    "mean, median, lower_25, upper_975"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean and median both indicate a central estimate of ECS of 3C, with a 95% credible interval of 2.1 to 3.9C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_index = np.searchsorted(ecs_values, 4.5)\n",
    "probability_ecs_gt_4_5 = np.trapezoid(final_posterior[threshold_index:], ecs_values[threshold_index:])\n",
    "probability_ecs_gt_4_5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of ECS being greater than 4.5C from this posterior is tiny (>> 1%)!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The posterior will be sensitive to both the specification of the prior and the selection (and specification) of the likelihood(s).  Play around with changing these components of the ECS posterior estimate.  Specifically:\n",
    "\n",
    "1. Use a uniform prior between 0 and 10 (see code below) - how sensitive is the result (mean, credible interval, probability of ECS>4.5C) to the choice of prior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a uniform prior PDF instead\n",
    "uniform_density = 1 / (ecs_values[-1] - ecs_values[0])  # Constant probability density for uniform distribution, but will need to integrate to 1 over 0 to 10\n",
    "uniform_prior_pdf = np.full_like(ecs_values, uniform_density) # because of the line above, PDF will integrate to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How might the posterior change with new information?  For instance, here are two papers which change the nature of the likelihoods used here:\n",
    "\n",
    "> Pauling, A.G., Bitz, C.M. and Armour, K.C., 2023. The climate response to the Mt. Pinatubo eruption does \n",
    "> not constrain climate sensitivity. Geophysical Research Letters, 50(7), p.e2023GL102946.\n",
    "\n",
    "> Tierney, J.E., Zhu, J., King, J., Malevich, S.B., Hakim, G.J. and Poulsen, C.J., 2020. Glacial cooling and\n",
    ">  climate sensitivity revisited. Nature, 584(7822), pp.569-573."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geog696fpython311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
