{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation complications and Monte Carlo methods\n",
    "\n",
    "When we left off last time, we were about to attempt to deal with the consequences of the non-independence of our data when calculating the significance value of Pearson's correlation.  When the values in our dataset are not independent of one another, this can bias our estimate of the significance of any correlation ($p$) and also potentially be misleading about the effect size ($r$).  Let's create some random Gaussian series of numbers and see how various forms of non-independence can affect them.\n",
    "\n",
    "First, let's get some libraries imported and see how we can create random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # import Numpy as np\n",
    "import pandas as pd # import Pandas as pd\n",
    "import scipy as sp # Import Scipy as sp\n",
    "import matplotlib.pyplot as plt # get the plotting functionality from Matplotlib\n",
    "\n",
    "# you can omit the line below if you'd like, but I really don't like the default fonts in Python, so I switch to Helvetica\n",
    "plt.rcParams['font.family'] = 'Helvetica'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that if our data points are not truly independent, this can bias our assessment of significance toward being too liberal (e.g. a Type 1 error).  Non-independence (especially if we think about it in _time_) manifests as auto-correlation (correlation between values in the data and other values in the same data) - In time series, we'll call this _serial_ autocorrelation :that consecutive values are not simply random draws from a distribution but rather are influenced in part by values that came before.  We can write a simple 1st order autocorrelation relationship as: \n",
    "\n",
    "$\n",
    "X_{t} = \\alpha_{1} X_{t-1} + Z_{t}\n",
    "$\n",
    "\n",
    "Or, for every given value $x$, that value is in part determined by the value that preceded it.  Now, let's think again about the equation for the t-statistic:\n",
    "\n",
    "$\n",
    "t = r \\sqrt{\\frac{n-2}{1-r^2}}\n",
    "$\n",
    "\n",
    "The $n$ is not simply the sample size, but rather accounts for how many degrees of freedom we have.  In situations where the values in our datasets are not independent, we have _fewer_ degrees of freedom than we have samples, and not accounting for this means that our $n$ is too large and therefore our $p$ will over-estimate the probability of rejecting the null hypothesis. \n",
    "\n",
    "Let's write a function to do a very simple calculation of the self- (auto) correlation in a time series.  We'll do that simply by creating a copy of the series and sliding (or lagging) it relative to itself, and calculating the correlation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagged_autocorrelation(series, lag=1): # default is lag of 1 - note, lag of zero will give an error here - how would you improve this function to avoid that? \n",
    "    # Calculate the Pearson correlation coefficient between the series and its lagged version (lag=1 is my default)\n",
    "    return  sp.stats.pearsonr(series[:-lag], series[lag:])[0] # returns correlation only with [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That function will now allow us to estimate the first order autocorrelation of a series (defaulting to the 1st lag), $\\rho_1$:\n",
    "\n",
    "$\n",
    "\\rho_1 = \\frac{\\sum_{t=1}^{N-1} (X_t - \\bar{X})(X_{t+1} - \\bar{X})}{\\sum_{t=1}^{N} (X_t - \\bar{X})^2}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create some autocorrelated series now using the equation:\n",
    "\n",
    "$\n",
    "X_{t} = \\alpha_{1} X_{t-1} + Z_{t}\n",
    "$\n",
    "\n",
    "Here we are creating synthetic or surrogate datasets to use for our lesson - we'll hear these terms more later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a 'random' seed to a specific number -- my age -- for reproducibility so we all get the same thing\n",
    "np.random.seed(1999) # Prince's 1982 album with such tracks as '1999' and 'Little Red Corvette'\n",
    "\n",
    "# Parameters for the autoregressive equation we'll write\n",
    "n = 128  # once again, length of the time series\n",
    "alpha1 = 0.9  # an autocorrelation coefficient (0 < alpha < 1)\n",
    "alpha2 = 0.7  # another autocorrelation coefficient (0 < alpha < 1)\n",
    "\n",
    "# Generate the first time series with autocorrelation\n",
    "xa = np.zeros(n)\n",
    "xa[0] = np.random.normal()  # Initialize the first value from the random normal\n",
    "for t in range(1, n): # loop over the length of the series, determining the next value from the combination of the autocorrelation and another random number\n",
    "    xa[t] = alpha1 * xa[t-1] + np.random.normal()\n",
    "\n",
    "# Generate the second time series with autocorrelation like above\n",
    "ya = np.zeros(n)\n",
    "ya[0] = np.random.normal() \n",
    "for t in range(1, n):\n",
    "    ya[t] = alpha2 * ya[t-1] + np.random.normal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two time series, $xa$ and $ya$ that have a degree of autocorrelation (although different for each one) - let's see what that actually looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two time series\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(xa, label='Time Series X', linestyle='-', marker='o')\n",
    "plt.plot(ya, label='Time Series Y', linestyle='-', marker='x')\n",
    "\n",
    "plt.title('Two Autocorrelated Time Series')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho1 = sp.stats.pearsonr(xa,ya) \n",
    "rho1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a very slight association (but still $p>0.05$).  If we generated enough different time series, we might find that eventually we get 'lucky' and find a set that are significantly correlated (which, remember, is one of the things we're scared of).  But for our purposes in the next section, all we need is to keep our eye on that p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling your own correlation for reduced degrees of freedom\n",
    "\n",
    "The function below will take two time series and the degrees of freedom.  If you thought there was no meaningful autocorrelation, you might make the effective sample size $N_{eff}$ the same as the actual sample size.  But as we'll see, we can pass this function a penalized 'effective' sample size to account for autocorrelation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_correlation_function(x,y,neff):\n",
    "    dof = neff - 2\n",
    "    correlation_matrix = np.corrcoef(x, y)\n",
    "    correlation_value = correlation_matrix[0, 1]\n",
    "    t_statistic = (correlation_value * np.sqrt(dof)) / np.sqrt(1 - correlation_value**2)\n",
    "    p_value = 2 * (1 - sp.stats.t.cdf(abs(t_statistic), df=dof)) \n",
    "    return correlation_value, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we get the same answer as Scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_correlation_function(xa,ya,np.size(xa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup! Looks like we did an OK job rolling out own!\n",
    "\n",
    "We're going to use a calculation of the effective samples size $N_{eff}$ from [Thiebaux and Zwiers 1984](https://journals.ametsoc.org/view/journals/apme/23/5/1520-0450_1984_023_0800_tiaeoe_2_0_co_2.xml), although it has also been described and articulated elsewhere.  The effective sample size is the sample size times the ratio of 1 less the product of the 1st order autocorrelation over 1 plus the product of the 1st order autocorrelations.  The effect here will be that the larger the autocorrelation, the smaller the numerator and the larger the denominator, and therefore the greater penalty:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "N_{\\rm eff} = N \\frac{(1-\\rho_1\\rho_2)}{(1+\\rho_1\\rho_2)}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho1 = lagged_autocorrelation(xa)\n",
    "rho2 = lagged_autocorrelation(ya)\n",
    "N = np.size(xa)\n",
    "\n",
    "Neff = round(N * (1 - rho1 * rho2) / (1 + rho1 * rho2))\n",
    "N, Neff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine arrays into a 2D array (column-wise)\n",
    "combined_array = np.column_stack((xa, ya))\n",
    "\n",
    "# Save the combined array to a text file\n",
    "np.savetxt('output.txt', combined_array, delimiter=',', header='Array1,Array2', fmt='%d', comments='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how using this much reduced effective sample size changes the p-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_correlation_function(xa,ya,Neff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient has remained the same, but notice the large difference in $p$ when we account for the reduced degrees of freedom. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above method works in many cases, it can be very conservative for strongly autocorrelated series.  And even in cases of less severe autocorrelation, [several authors](https://journals.ametsoc.org/view/journals/apme/23/5/1520-0450_1984_023_0800_tiaeoe_2_0_co_2.xml) have pointed out the challenge of getting an accurate estimate of the true $N_{eff}$ using this equation.  This approach may also fail if the autocorrelation structure is much more complicated than a 1st order lag.  \n",
    "\n",
    "An alternative approach is to simulate our time series, creating random series that have some of the properties of our real data that we are concerned might be interfering with making a good inference. 'The idea behind resampling techniques is to generate random series that have properties similar to the original series, and to derive a confidence interval using these random series' ([Ebisuzaki 1997](https://journals.ametsoc.org/view/journals/clim/10/9/1520-0442_1997_010_2147_amtets_2.0.co_2.xml)). These simulation exercises belong to a larger class of statistical operations called _Monte Carlo_ simulations.  In lecture we'll discuss more about the history of these methods and their application.\n",
    "\n",
    "Here we'll look specifically at the approach described by [Wesley Ebisuzaki](https://journals.ametsoc.org/view/journals/clim/10/9/1520-0442_1997_010_2147_amtets_2.0.co_2.xml) in his 1997 paper in the _Journal of Climate_, which performs the simulation of our time series using _phase randomization_. \n",
    "\n",
    "In phase randomization, the surrogate or simulated time series are generated base by resampling in the _frequency_ domain, thereby capturing not only short-lag autocorrelation, but also potentially other longer-term or more complex fluctuations.  The result of the procedure are a random series with the same cyclical structure of the original series, but out of phase -- or to look at it a different way, we've scrambled the series so they still have some of the important properties of the original dataset that we are concerned might be interfering with our statistical test and inferences, but with any true relationship degraded by randomizing the temporal order.  The Ebisuzaki paper above describes this in more detail.  \n",
    "\n",
    "Once phase randomized series have been created, you can calculate the correlation between these series.  If we do this over and over again (the Monte Carlo part of the procedure) we can create a distribution of possible correlation values for random (unrelated) series with properties similar to the real data.  If the correlation of our real data exceeds some number of those surrogate random tests (let's say, if our correlation is larger than 950 of the 1000 simulations), we \n",
    "\n",
    "We're going to write a function below that performs the Ebisuzaki test.  We need it to do a few things.  We need to calculate the actual correlation between our series, we need to then estimate the (possibly complicated) autocorrelation structure of our time series, we need to create phase randomized versions of our time series, calculate their correlations (and store them for later), and then assess the correlation for the actual data against the distribution of surrogate correlation values from our simulation.  To do so, we'll use the phase randomization method from [Prichard and Theiler 1994](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.73.951).\n",
    "\n",
    "Let's put together our Ebisuzaki function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def ebisuzaki(x, y, level=0.05, nsim=1000, seed=None):\n",
    "    \"\"\"\n",
    "    Ebisuzaki's method (1997) for estimating significant correlation values for autocorrelated time series.\n",
    "    \n",
    "    Inputs:\n",
    "    x (array): First time series vector.\n",
    "    y (array): Second time series vector.\n",
    "    sig (float): Significance level for critical value estimation, e.g. level=0.05 or level=0.01\n",
    "    nsim (int): Number of simulations - tested for 1000 and 10000 and quite fast for either\n",
    "    seed: Can be used for reprodicibility or to force Numpy to randomize the seed\n",
    "\n",
    "    Returns:\n",
    "    F (float): The fraction (from 0 to 1) of time series with higher correlation coefficients (done as a two tailed test)\n",
    "    rSim (ndarray): The array of simulated correlation coefficients, which we can use for plotting\n",
    "    r_real (float): The observed correlation between the actual data series x and y (for plotting)\n",
    "    \n",
    "    \n",
    "    Citations:\n",
    "    \n",
    "    Ebisuzaki, W, 1997: A method to estimate the statistical significance of a correlation \n",
    "    when the data are serially correlated, Journal of Climate, 10, 2147-2153.\n",
    "    \n",
    "    Prichard, D, Theiler, J, 1994: Generating Surrogate Data for Time Series\n",
    "    with Several Simultaneously Measured Variables, Physical Review Letters, 73, 7, 951-954.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the random seed if provided (e.g. for reproducibility), otherwise ask Numpy to randomize it\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    else:\n",
    "        np.random.seed(None)  # Randomizes the seed\n",
    "    \n",
    "    # Convert to numpy arrays and standardize using zscore\n",
    "    x = zscore(np.asarray(x).flatten())\n",
    "    y = zscore(np.asarray(y).flatten())\n",
    "\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"Size of x and y must be the same\")\n",
    "\n",
    "    n = len(x)\n",
    "    n2 = n // 2 # use this for the phase randomization\n",
    "\n",
    "    # Compute the real correlation coefficient\n",
    "    r_real = np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "    # Compute FFT and their magnitudes\n",
    "    xf = np.fft.fft(x) # FFT of x\n",
    "    yf = np.fft.fft(y) # FFT of y\n",
    "    modx = np.abs(xf) # just the positive values\n",
    "    mody = np.abs(yf) # just the positive values\n",
    "    \n",
    "    # Initialize the arrays for surrogates with the right length (n) and the right number of surrogate series (nsim)\n",
    "    X = np.zeros((n, nsim))\n",
    "    Y = np.zeros((n, nsim))\n",
    "    \n",
    "    # Now, generate surrogates with random phases, looping over the number of surrogates (this could probably be more efficient, but is still reasonable quick)\n",
    "    # Based on Ebisuzaki 1997 and on Prichard and Theiler 1994\n",
    "    for i in range(nsim):\n",
    "        # Generate random phases for each surrogate series \n",
    "        phases_x = np.random.uniform(-np.pi, np.pi, n2 - 1)  # random phase for x\n",
    "        phases_y = np.random.uniform(-np.pi, np.pi, n2 - 1)  # random phase for y\n",
    "        \n",
    "        # Construct full phase arrays for both the even and odd cases for x, follows Prichard and Theiler 1994\n",
    "        if n % 2 == 0: # even case\n",
    "            phases_full_x = np.concatenate(([0], phases_x, [0], -phases_x[::-1]))\n",
    "            phases_full_y = np.concatenate(([0], phases_y, [0], -phases_y[::-1]))\n",
    "        else: # odd case\n",
    "            phases_full_x = np.concatenate(([0], phases_x, -phases_x[::-1]))\n",
    "            phases_full_y = np.concatenate(([0], phases_y, -phases_y[::-1]))\n",
    "        \n",
    "        # Create surrogates by applying the random phases to the FFT\n",
    "        recf_x = modx * np.exp(1j * phases_full_x)\n",
    "        recf_y = mody * np.exp(1j * phases_full_y)\n",
    "        \n",
    "        # Now, we get the inverse to get our surrogated back into the time domain \n",
    "        X[:, i] = np.real(np.fft.ifft(recf_x))\n",
    "        Y[:, i] = np.real(np.fft.ifft(recf_y))\n",
    "\n",
    "    # Standardize the surrogates using zscore which simplifies the matrix algebra if we wanted to take that approach\n",
    "    X = zscore(X, axis=0)\n",
    "    Y = zscore(Y, axis=0)\n",
    "\n",
    "    # Calculate the correlations for each pair of surrogates - this is fast enough using the line comprehension so we'll do it this way\n",
    "    rSim = np.array([np.corrcoef(X[:, i], Y[:, i])[0, 1] for i in range(nsim)])\n",
    "    \n",
    "    # alternatively, I might have used the dot product for this, which is also efficient - take GEOG696C if you want to see why!  \n",
    "    # rSim = np.diag(np.dot(X.T, Y)) / (n - 1)\n",
    "    \n",
    "    # now, calculate F as the fraction (so, 0 to 1) of the surrogate tests that have a larger (two tailed) correlation than the actual correlation of the real series\n",
    "    F = np.sum(np.abs(rSim) > np.abs(r_real)) / nsim # note this is effectively a two tailed test - we are allowing for larger magnitude either positive or negative\n",
    "\n",
    "    # We can also find critical value needed to find a fraction of 0.05 or smaller\n",
    "    rSims_sorted = np.sort(np.abs(rSim))\n",
    "    cv = rSims_sorted[int(nsim * (1 - level))] # essentially, what value is in the position to allow for fraction of only 0.05 to be larger (two tailed)\n",
    "\n",
    "    # Output results\n",
    "    print('---')\n",
    "    print(f'Observed correlation coefficient: r = {r_real:.3f}')\n",
    "    print(f'Critical R Value: r = {cv:.3f}')\n",
    "    print(f'Fraction of coefficients larger than observed: {F:.3f}')\n",
    "\n",
    "    return F, rSim, r_real\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the function on the two series we created above.  We can run with 1000 simulations to start, but 10,000 runs quite quickly on my computer as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F, rSim, r_real = ebisuzaki(xa, ya, level=0.05, nsim=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to interpret the text output from this function?  The two series we generated for this exercise as simple lag-1 autoregressive processes gave a 'real' correlation or $r=0.131$.  Over 10,000 simulations, the phase randomized series produced _larger_ r values than that about 36% of the time (this is what is in the third line or text output).  The second line of the output tells us we actually needed a 'real' $r$ of 0.269 or higher to be significant at $p<0.05$ (e.g. compared to the $r=0.131 we observed from the 'real' data).\n",
    "\n",
    "We can visualize the result of these simulations using a plot as well.  Let's draw the histogram of the 10,000 simulated correlations as well as the 'real' observed value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rSim,label='Simulated');\n",
    "plt.axvline(r_real,linestyle='--',color='r',label='Observed');\n",
    "plt.xlabel('Correlation Coefficients (r)')\n",
    "plt.ylabel('Frequency');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative way to visualize the two tailed test\n",
    "plt.hist(np.abs(rSim),label='Simulated');\n",
    "plt.axvline(np.abs(r_real),linestyle='--',color='r',label='Observed');\n",
    "plt.xlabel('Correlation Coefficients (r)')\n",
    "plt.ylabel('Frequency');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our real value falls well within the distribution of the simulated correlations.   Indeed, random series with the same autocorrelation structure of our series but different phase can produce correlation values up to 0.40 just by chance (clowns, again).  \n",
    "\n",
    "We might interpret this then as meaning that the observed correlation we calculated is not that unusual in the context of random (unrelated) series with similar autocorrelation structure.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "1. Be sure to review the Jupyter notebook `loops_in_python` in this same directory\n",
    "2. What other ways could you imagine conducting a Monte Carlo simulation to test for significance (e.g. what other things might we be afraid of that we want to test?) - you should consult the documentation for the [`itertools`](https://docs.python.org/3/library/itertools.html), which provides many ways to iterate or loop.  What kind of simulations might you accomplish with some of these other tools? \n",
    "3. Imagine a situation where you were correlated a single time series of data against a spatial dataset (e.g. National GDP vs. county-level GDP) - what other things might you be concerned about if you were interested in determining which counties were significant correlated with national GDP figures? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
