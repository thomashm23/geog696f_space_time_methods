{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis I\n",
    "\n",
    "It is quite common that our data with be in the form of time-ordered observations or data points.  While we can often treat these data without addressing the time component itself (e.g. correlation between two time series, for instance, doesn't know anything about 'time', just the pairing of the $X$ and $Y$ values), the existence of a temporal order and potential temporal relationships (covariance, etc.) does present some unique issues and concerns (as we'll discuss briefly in lecture).  There are entire courses on time series analysis and we'll just scratch the surface here.  \n",
    "\n",
    "In this notebook, we'll look at stationarity and the decomposition of time series into trend, seasonality, and residual components using Python and specifically statsmodels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity\n",
    "\n",
    "Let's start by looking at a simple example of a stationary vs. non-stationary time series (for a discussion of this specifically with respect to statsmodels, see [here](https://www.statsmodels.org/dev/examples/notebooks/generated/stationarity_detrending_adf_kpss.html). First, let's get some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# you can omit the line below if you'd like, but I really don't like the default fonts in Python, so I switch to Helvetica\n",
    "plt.rcParams['font.family'] = 'Helvetica'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now create two time series.  Recall from lecture that a Gaussian (white) noise time series will be stationary -- it will have a mean, variance, and other statistical properties (including a lack of temporal covariance) that do not change and are not dependent on time.  We can also create a non-stationary time series by calculating a cumulative sum of that time series.  This is essentially a discrete random walk or Brownian motion, since there is a strong dependence at any time step to the value of the earlier data and the summation process.  We would a prior expect this to be non-stationary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1999)\n",
    "n_samples = 128\n",
    "\n",
    "# make a random normal (Gaussian) time series - this will be stationary\n",
    "data1 = pd.Series(np.random.randn(n_samples))\n",
    "\n",
    "# calculate the cumulative sum of a random Gaussian series - this will be non-stationary\n",
    "data2 = pd.Series(np.cumsum(data1))\n",
    "\n",
    "# plot both series to see what they look like, each on a single subplot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "ax[0].plot(data1)\n",
    "ax[0].set_title('Random Series (Stationary)')\n",
    "ax[0].set_xlabel('Time')\n",
    "ax[0].set_ylabel('Value')\n",
    "\n",
    "ax[1].plot(data2)\n",
    "ax[1].set_title('Cumulative Sum (Non-Stationary)')\n",
    "ax[1].set_xlabel('Time')\n",
    "ax[1].set_ylabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call the Augmented Dickey-Fuller (ADF) test from statsmodels ([see here](https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.adfuller.html)).  The test returns various results, but we'll want to look at the 0th (the ADF statistic itself), the 1st (the p value), and the 4th (which shows the critical values, providing some context for the ADF statistic which is otherwise somewhat abstracted from the data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the the Augmented Dickey-Fuller (ADF) test module from statsmodels\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Perform the Augmented Dickey-Fuller (ADF) test on the random normal time series\n",
    "result1 = adfuller(data1)\n",
    "\n",
    "print('ADF Statistic:', result1[0])  # compare to the critical values below\n",
    "print('p-value:', result1[1]) # this will be very SMALL, so we REJECT the null hypothesis of non-stationarity - e.g the series is probably STATIONARY\n",
    "print('Critical Values:', result1[4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the p value is very very small, which would lead us to REJECT the null hypotheses.  Somewhat oddly, the null in this case is that the time series is _non-stationary_, so rejecting that null means the series is most likely stationary (I have to think this through each time I do it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Augmented Dickey-Fuller (ADF) test on the cumulative sum time series\n",
    "result2 = adfuller(data2)\n",
    "\n",
    "print('ADF Statistic:', result2[0]) # compare to the critical values below\n",
    "print('p-value:', result2[1]) # this will be LARGER than p=0.05, so we FAIL TO REJECT the null hypothesis of non-stationarity - e.g the series is probably NON-STATIONARY\n",
    "print('Critical Values:', result2[4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that the random walk (cumulative sum) time series has a value larger than p=0.05, so we might feel comfortable NOT rejecting the null, and concluding the series is most likely non-stationary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mauna Loa CO2 record\n",
    "\n",
    "Below we're going to use the monthly atmospheric CO2 record from Mauna Loa (Hawaii) to look at ways we can use Python for time series decomposition and analysis.  The data come from [here](https://gml.noaa.gov/ccgg/trends/) and specifically [here](https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.txt).  The monthly data made available by NOAA are the average of much more frequent observations that have been quality controlled. \n",
    "\n",
    "First, let's read in the data into a Pandas DataFrame and take a look: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('co2_mm_mlo.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An aside: using `datetime` in Pandas and beyond\n",
    "\n",
    "Numpy, Pandas, and xarray (amongst other libraries) have the ability to use `datetime64` and `timedelta64` types to assist with indexing temporal data (there are some limitations, however, that climate scientists should be aware of, see: https://discourse.pangeo.io/t/pandas-dtypes-now-free-from-nanosecond-limitation/3106).  There is a very clear discussion of this with respect to Pandas [here](https://pandas.pydata.org/docs/user_guide/timeseries.html). As we will see if greater detail later in the class, this provides better ways to index time series data, but also to automate processes like cross-correlation.  \n",
    "\n",
    "Here, we're going to do a very simple datetime operation.  As you see above, the data from Mauna Loa comes with a year and month column as well as a decimal year column.  For our purposes but also for practice, let's see how we can turn the 'year' and 'month' columns (which are just integers at the moment) into a true datetime value that Pandas recognizes as providing a set of calendar dates.\n",
    "\n",
    "First, let's use Panda's indexing to create a new column called `date` in our DataFrame and populate it with a datetime consisting of the year column, the month column, and add a day.  By adding a day (I've used day=1, the first day of the month, but you could use e.g. day=15 the middle of the month, etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine 'year' and 'month' into a single new datetime column called 'date'\n",
    "df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))  # required to have 'day=1' here, which makes it a proper datetime value \n",
    "df.info() # note that the date has been created as datetime64, but the index still says 'RangeIndex: 780 entries, 0 to 779'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will set the DataFrame's index (which is currently just the row number) to use the new date column we created.  We use `inplace=True` so that this changes the DataFrame itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now set the 'date' column as the index\n",
    "df.set_index('date', inplace=True)\n",
    "df.info() # note that the index now says 'DatetimeIndex: 780 entries, 1959-01-01 to 2023-12-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's drop the year and month columns, since they've served their purpose.  I'm going to leave the decimal_data for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can drop the columns that we aren't going to use - note you need to specify 'axis' for this drop operation\n",
    "df.drop(['year', 'month'], axis=1, inplace=True) \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our data now.  The data column co2 has an index which is a datetime `date` as you see above, so Matplotlib will automatically use that datetime for the x axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['co2'],'k')\n",
    "plt.xlabel('YEAR')\n",
    "plt.ylabel('CO2 (ppm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, let's do our stationarity test, although with a trend and seasonal component we should already anticipate the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_test = adfuller(df['co2'])\n",
    "\n",
    "print('ADF Statistic:', stationarity_test[0]) # compare to the critical values below\n",
    "print('p-value:', stationarity_test[1]) # this will be (much!) larger than p=0.05, so we fail to reject the null hypothesis of non-stationarity - e.g the series is non-stationary\n",
    "print('Critical Values:', stationarity_test[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to use statsmodels' `seasonal_decompose` from its time series analysis sub-library (see [here](https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html)).  This module performs a very simple operation of identifying the trend and seasonal components and calculating the residual.  This can be useful for exploratory data analysis or for filtering, something we'll talk about on Wednesday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "decomposition = seasonal_decompose(df['co2'], model='additive', extrapolate_trend='freq', period=12)\n",
    "\n",
    "fig = decomposition.plot() # by assigning this to a figure object, we can edit the size of the figure from the default, which is hard to read IMO\n",
    "fig.set_size_inches((5, 8))\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this code has been run, we can extract the components (or the results in general) from the `decomposition` object. For instance, here is the mean annual cycle, which we get by averaging each month (remember, datetime tells Python which month each data point corresponds to) from the seasonal time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the seasonal component\n",
    "seasonal = decomposition.seasonal\n",
    "\n",
    "# Calculate the mean seasonal cycle\n",
    "mean_seasonal_cycle = seasonal.groupby(seasonal.index.month).mean()\n",
    "\n",
    "# Plot the mean seasonal cycle\n",
    "months = ['January', 'February', 'March', 'April', 'May', 'June', \n",
    "          'July', 'August', 'September', 'October', 'November', 'December']\n",
    "plt.plot(mean_seasonal_cycle,'-o')\n",
    "plt.xlabel(\"MONTH\")\n",
    "plt.xticks(ticks=range(1, 13), labels=months, rotation=45)\n",
    "plt.ylabel(\"Mean Seasonal Component (CO2 ppm)\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do something similar with the trend component.  As an example of what we might do with this, let's use Numpy to fit a linear and 2nd order polynomial line to the trend component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the trend component\n",
    "trend = decomposition.trend\n",
    "time = df['decimal_date']\n",
    "\n",
    "# Fit a linear trend (degree 1)\n",
    "linear_fit = np.polyfit(time, trend, deg=1) # first order (linear) polynomial fit\n",
    "linear_trend = np.polyval(linear_fit, time)\n",
    "\n",
    "# Fit a quadratic trend (degree 2)\n",
    "quadratic_fit = np.polyfit(time, trend, deg=2) # 2nd order polynomial fit\n",
    "quadratic_trend = np.polyval(quadratic_fit, time)\n",
    "\n",
    "# Plot the linear trend\n",
    "plt.plot(time, trend, label='CO2 Trend Component', color='black')\n",
    "plt.plot(time, linear_trend, label='Linear Model Fit', color='red',linestyle='--')\n",
    "plt.plot(time, quadratic_trend, label='Quadratic Model Fit', color='blue',linestyle='--')\n",
    "plt.xlabel(\"YEAR\")\n",
    "plt.ylabel(\"CO2 (ppm)\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a bit of simple maths here to see which of the fits has the lowest residual (or error).  This kind of simple calculation could be used to determine or justify an interpretation of the trend -- in this case, is the trend in atmospheric CO2 linear, or is it accelerating? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual sum of squares for linear fit\n",
    "rss_linear = np.sum((trend - linear_trend) ** 2)\n",
    "\n",
    "# Residual sum of squares for quadratic fit\n",
    "rss_quadratic = np.sum((trend - quadratic_trend) ** 2)\n",
    "\n",
    "print(f'Residual sum-of-squares for linear fit: {rss_linear}')\n",
    "print(f'Residual sum-of-squares for quadratic fit: {rss_quadratic}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the quadratic fit is a better fit to the data, which could support an interpretation that there has been an acceleration in the rate of CO2 increase. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we make this record stationary? \n",
    "\n",
    "We saw above that the CO2 record is non-stationary (no surprise, given the obvious trend and seasonality components).  As discussed in class, there are ways we can attempt to make a series stationary.  Here, we'll do three things:  First, we'll take the log of the CO2 series - if there is a chance in variance through time, this can address this component of non-stationarity.  Second, we'll take the first difference, which will remove the simple trend.  Finally, we'll do another differencing where each point is differenced relative to the point 12 values (12 months) before it.  This will remove the seasonal cycle (note that this is not the only way, or even the best way in some cases, to do this, but it does demonstrate a nice feature of the `.diff` function!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformation to stabilize variance - question for you?  Does the variance change at Mauna Loa?  \n",
    "co2_log = np.log(df['co2'])\n",
    "\n",
    "# First differencing to remove the trend\n",
    "co2_diff = co2_log.diff()\n",
    "\n",
    "# month-to-month differencing to remove remaining seasonal elements - note this will create 12 NaN values so we use .dropna()\n",
    "co2_seasonal_diff = co2_diff.diff(12).dropna()\n",
    "\n",
    "# Plotting the transformed data (after differencing)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(co2_seasonal_diff, label='Stationary CO2 Data')\n",
    "plt.title('Stationary CO2 Data After Differencing')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Differenced Log CO2 Concentration')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did these varios transformations make the CO2 series stationary? Let's use the Augmented Dickey-Fuller test one more time to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Augmented Dickey-Fuller test\n",
    "co2_adf_result = adfuller(co2_seasonal_diff)\n",
    "print(f'ADF Statistic: {co2_adf_result[0]}')\n",
    "print(f'p-value: {co2_adf_result[1]}') # very very small, can REJECT null hypothesis, which means series is likely STATIONARY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
