{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation and complications, part II\n",
    "\n",
    "In the previous notebook, we talked about some of the assumptions or requirements for (Pearson) correlation.  To review, we assume that:\n",
    "\n",
    "1. The variables we are analyzing are continuous (vs. discrete, categorical, or ordinal) -- they can take on any value in a range of numbers\n",
    "2. The relationship between the variables is expected to be linear (vs. nonlinear, for which we'll need a different test)\n",
    "3. The variables come from a normal distribution\n",
    "4. The are no outliers in the dataset\n",
    "\n",
    "In the real world, our data may violate some (or all!) of these assumptions.  In some cases this indicates that the Pearson correlation is not the appropriate test (see below for dealing with non-linear and non-continuous variables).  In other cases we may need to account for these violations either through modifying (penalizing) our effect size or taking a different approach to calculating significance.  \n",
    "\n",
    "We have to add another assumption as well (and this particularly affects our judgement and calculation of significance) about the independence of the values in our datasets:\n",
    "\n",
    "5. The variables are random and independent \n",
    "\n",
    "This may be violated in a number of ways in real data, but one of the ones we are most concerned about will be issues of spatial and temporal autocorrelation -- that some of our data are dependent on others.  In the next class we'll explore the consequences of this and some of the ways to deal with this.  We also need to be mindful of how sample size (and more generally degrees of freedom) affect significance testing. If you look at the calculation of the t statistic above, you see the degrees of freedom are in the numerator, and so very large samples sizes can give significant results for relatively small effect sizes (small values of _r_).  Evaluating whether a significant p-value with a small effect size is 'meaningful', even if 'significant' is important.  This and other drawbacks of p-values has led to a large number of papers questioning their use and value (e.g. [see here](https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913)) over the last several years.  We also have to be aware of the statistical consequences of multiple comparisons (running many correlation analyses). \n",
    "\n",
    "Let's explore some of these challenges and some of the possible solutions.  We'll talk about other non-parametric approaches next week.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-independence\n",
    "\n",
    "When the values in our dataset are not independent of one another, this can bias our estimate of the significance of any correlation ($p$) and also potentially be misleading about the effect size ($r$).  Let's create some random Gaussian series of numbers and see how various forms of non-independence can affect them.\n",
    "\n",
    "First, let's get some libraries imported and see how we can create random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # import Numpy as np\n",
    "import pandas as pd # import Pandas as pd\n",
    "import scipy as sp # Import Scipy as sp\n",
    "import matplotlib.pyplot as plt # get the plotting functionality from Matplotlib\n",
    "\n",
    "# you can omit the line below if you'd like, but I really don't like the default fonts in Python, so I switch to Helvetica\n",
    "plt.rcParams['font.family'] = 'Helvetica'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy (amongst other libraries, of course) has extensive capacity for (pseudo) random number creation.  You can see more about that here, [Random sampling in Numpy](https://numpy.org/doc/stable/reference/random/index.html).  For the moment, we'll just create random normal data -- data drawn from a normal distribution with a given mean and variance.  Keep in mind that we will all be creating different sets of random numbers, so you won't get the exact answer as your classmates or the instructor, and if you run through the notebook multiple times you will get somewhat different numbers each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0\n",
    "std_dev = 1\n",
    "\n",
    "# the line of code below creates a random draw of 128 numbers from a normal distribution with mean of zero and standard deviation or 1 \n",
    "x = np.random.normal(mean, std_dev, 128)\n",
    "\n",
    "# Plot a histogram of the generated data\n",
    "plt.hist(x, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.title('Histogram of Random Data')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, with a relatively small number of random draws, the distribution looks roughly normal, but we would need a lot more draws to really look completely like the normal distribution itself. \n",
    "\n",
    "Let's create another random time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.normal(mean, std_dev, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a scatterplot of our two variables - we would expect them to appear as a cloud of points with little relationship between the pairs of datapoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.scatter(x, y, color='blue', marker='o')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Scatter plot of x and y')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then let's calculate the correlation between them.  Again, based on how we created the variables and the lack of relationship we see in the scatter plot, we might expect that given two random draws from a normal distribution that there should be no significant correlation between these two series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_correlation = sp.stats.pearsonr(x,y) \n",
    "print(np.round(random_correlation,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from Scipy's Pearson correlation function indeed shows $r=0.01$ and a p-value of 0.9.  In otherwords, a very weak relationship where we cannot reject the null hypothesis.  We would call this relationship 'not significant', are more precisely meaning that there is a 90\\% chance of observing $r=0.01$ or larger for the null hypothesis of no significant relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend effects on correlation\n",
    "\n",
    "OK, let's see how adding something as simple as a trend can affect that correlation value.  First, we'll generate a simple trend line with a given slope, add it to our random normal variables, and re-calculate the correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the slope\n",
    "slope = 0.05\n",
    "\n",
    "# Generate t values (0 to 127 in this case)\n",
    "t = np.arange(128)\n",
    "\n",
    "# Generate y values using that slope (y = slope * t) to give a line\n",
    "linear = slope * t\n",
    "\n",
    "x_lin = x + linear\n",
    "y_lin = y + linear\n",
    "\n",
    "trend_correlation = sp.stats.pearsonr(x_lin,y_lin) \n",
    "trend_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa, what happened - we added a very gentle upward sloping line to both random (and uncorrelation) series, and now we have a substantially correlation ($r=0.79) and a highly significant p value.  Let's take a look at our series and think about what has happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two series\n",
    "plt.plot(t, x_lin, label='x_lin', linestyle='-', marker='o')\n",
    "plt.plot(t, y_lin, label='y_lin', linestyle='-', marker='x')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Plot of Two Series: x_lin and y_lin')\n",
    "plt.xlabel('Time (t)')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Show grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting!  So even with a relatively shallow trend line added to the random, the correlation becomes large and highly significant.  In class now, let's talk about why that is.  If you'd like, play around with different values of `slope` above to see how the steepness of the trend line might affect the magnitude of the correlation and the significance level. \n",
    "\n",
    "Let's make a scatterplot to see how that has affected the structure of the data relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.scatter(x_lin, y_lin, color='blue', marker='o')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Scatter plot of x and y')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - the presence of a trend has created a structure that means that positive values of x+trend are now paired with positive values of y+trend. What about if the lines have different underlying trends?  That is, imagine a case where there are two different processes controlling long-term trends in our data series, each with a different magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate y values using that slope (y = slope * t) to give a line\n",
    "linear_1 = 0.01 * t\n",
    "linear_2 = 0.05 * t\n",
    "\n",
    "x_lin1 = x + linear_1\n",
    "y_lin1 = y + linear_2\n",
    "\n",
    "trend_correlation = sp.stats.pearsonr(x_lin1,y_lin1) \n",
    "trend_correlation # most likely still significant, although lower correlation r value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two series\n",
    "plt.plot(t, x_lin1, label='x_lin', linestyle='-', marker='o')\n",
    "plt.plot(t, y_lin1, label='y_lin', linestyle='-', marker='x')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Plot of Two Series: x_lin and y_lin')\n",
    "plt.xlabel('Time (t)')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Show grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.scatter(x_lin1, y_lin1, color='blue', marker='o')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Scatter plot of x and y')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our random series are correlated (although more weakly, and with less obvious structure) even with trends of different magnitudes.  \n",
    "\n",
    "Note that $r$ here in both cases is still _correct_ -- the relationship being described mathematically is essentially that when one of our series+trend is positive, the other tends to be positive, and when one is negative, the other tends to be negative.  So the $r$ is correctly calculated but the inference we draw from that statistic could be biased by the presense of the trend.  Our p value will be misleading as well. Remember that we get the t-statistic from the $r$ and the degrees of freedom ($n$) -- _but_ the degrees of freedom are only equivalent to the sample size when the values in our two datasets are independent and randomly draw from the same distribution.  In the case of a linear trend, that independence breaks down (more 'recent' values in our data series are more likely to be positive than 'earlier' values).  We'll need to think about how to potentially correct our $p$ value for the non-independence of the values in our dataset. \n",
    "\n",
    "Take a moment and chat with your classmates - what is one way we might deal with this problem?  We'll put our answer below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here for a simple way to deal with trend effects\n",
    "\n",
    "from scipy.signal import detrend\n",
    "\n",
    "x_detrended = detrend(x_lin1)\n",
    "y_detrended = detrend(y_lin1)\n",
    "\n",
    "trend_correlation = sp.stats.pearsonr(x_detrended,y_detrended) \n",
    "trend_correlation # most likely still significant, although lower correlation r value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original_correlation = sp.stats.pearsonr(x,y)\n",
    "original_correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclical behavior\n",
    "\n",
    "Cyclical behavior can impart a similar structure to otherwise random data.  For instance, let's say you were interested in the peak of summer warmth in a strongly cyclical series like the annual cycle of temperature.  Similar cyclical patterns like the annual cycle can affect the independence of your data series and once again potentially lead to weakly supported inferences.  Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclesPerTime = 6 # how many cycles would we like to create per our 128 time steps? \n",
    "cycle = np.sin(2 * np.pi * (cyclesPerTime / 128) * t) # create a sine wave with that many cycles\n",
    "\n",
    "plt.plot(t,cycle) # let's take a look\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the cycle to our random series from before and see how this affects our correlation values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cycle = x + cycle\n",
    "y_cycle = y + cycle\n",
    "\n",
    "cycle_correlation = sp.stats.pearsonr(x_cycle,y_cycle) \n",
    "cycle_correlation # most likely still significant, although lower correlation r value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.scatter(x_cycle, y_cycle, color='blue', marker='o')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Scatter plot of x and y')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the presence of a regular cycle of positive and negative values has imparted an underlying relationship to the random variability of $x$ and $y$.  The solution here could be a bit more complicated than above with the linear trend, but you could imagine one simple approach would be calculating and removing a mean annual cycle from the data before doing any calculations on the underlying variability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation and independence \n",
    "\n",
    "Even when the underlying structure is not something as clear or dominant as a trend or cycle, if our data points are not truly independent, this can bias our assessment of significance toward being too liberal (e.g. a Type 1 error).  Non-independence (especially if we think about it in _time_) manifests as auto-correlation (correlation with yourself) - that is, that consecutive values are not simply random draws from a distribution but rather are influenced in part by values that came before.  We can write a simple 1st order autocorrelation relationship as: \n",
    "\n",
    "$\n",
    "X_{t} = \\alpha_{1} X_{t-1} + Z_{t}\n",
    "$\n",
    "\n",
    "Or, for every given value $x$, that value is in part determined by the value that preceded it.  Now, let's think again about the equation for the t-statistic:\n",
    "\n",
    "$\n",
    "t = r \\sqrt{\\frac{n-2}{1-r^2}}\n",
    "$\n",
    "\n",
    "The $n$ is not simply the sample size, but rather accounts for how many degrees of freedom we have.  In situations where the values in our datasets are not independent, we have _fewer_ degrees of freedom than we have samples, and not accounting for this means that our $n$ is too large and therefore our $p$ will over-estimate the probability of rejecting the null hypothesis. \n",
    "\n",
    "Let's write a function to do a very simple calculation of the self- (auto) correlation in a time series.  We'll do that simply by creating a copy of the series and sliding (or lagging) it relative to itself, and calculating the correlation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagged_autocorrelation(series, lag=1): # default is lag of 1 - note, lag of zero will give an error here - how would you improve this function to avoid that? \n",
    "    # Calculate the Pearson correlation coefficient between the series and its lagged version (lag=1 is my default)\n",
    "    return  sp.stats.pearsonr(series[:-lag], series[lag:])[0] # returns correlation only with [0]\n",
    "\n",
    "# now, call the function we wrote and apply to the combination of random x and the sine wave cycle\n",
    "lagged_autocorrelation(x_cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that at lag 1, the time series `x_cycle` is relatively similar to itself - there is information shared between any given value in the series and value that precedes it, meaning the series does _not_ consist of completely independent values and the degrees of freedom are not as large as the sample size.\n",
    "\n",
    "We can calculate and plot the correlation at various lags - in the first few lines below, I demonstrate a [list comprehension](https://realpython.com/list-comprehension-python/) -- essentially, a replacement for a loop.  Then we'll use Matplotlib to plot the correlation as a function of the lag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = range(1, 21)\n",
    "autocorrelations = [lagged_autocorrelation(x_cycle, lag) for lag in lags] # this is a list comprehension, see here: https://realpython.com/list-comprehension-python/\n",
    "\n",
    "# note that the alternative to the list comprehension above is the following loop - it works as well but is longer! \n",
    "# lags = range(1, 21)\n",
    "# autocorrelations = []  # Initialize an empty list to store results\n",
    "# for lag in lags:\n",
    "#    autocorr = lagged_autocorrelation(x_cycle, lag)  # Calculate the autocorrelation for the current lag\n",
    "#    autocorrelations.append(autocorr)  # Append the result to the growing list\n",
    "\n",
    "# Plot the autocorrelation values\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.stem(lags, autocorrelations)\n",
    "plt.xticks(lags)  # Set x-ticks to show only the lags from 1 to 20\n",
    "plt.xlim(0, 20)  # Limit x-axis to the range 1 to 20\n",
    "plt.title('Lagged Autocorrelation (x + cycle)')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some autocorrelated series now using the equation:\n",
    "\n",
    "$\n",
    "X_{t} = \\alpha_{1} X_{t-1} + Z_{t}\n",
    "$\n",
    "\n",
    "In this case we won't have the severity or structure that imparting a cycle had in the example above, but we are still going to reduce the independence and therefore the degrees of freedom in our series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a 'random' seed to a specific number -- my age -- for reproducibility so we all get the same thing\n",
    "np.random.seed(48)\n",
    "\n",
    "# Parameters for the autoregressive equation we'll write\n",
    "n = 128  # once again, length of the time series\n",
    "alpha1 = 0.8  # an autocorrelation coefficient (0 < alpha < 1)\n",
    "alpha2 = 0.8  # another autocorrelation coefficient (0 < alpha < 1)\n",
    "\n",
    "# Generate the first time series with autocorrelation\n",
    "xa = np.zeros(n)\n",
    "xa[0] = np.random.normal()  # Initialize the first value from the random normal\n",
    "for t in range(1, n): # loop over the length of the series, determining the next value from the combination of the autocorrelation and another random number\n",
    "    xa[t] = alpha1 * xa[t-1] + np.random.normal()\n",
    "\n",
    "# Generate the second time series with autocorrelation like above\n",
    "ya = np.zeros(n)\n",
    "ya[0] = np.random.normal() \n",
    "for t in range(1, n):\n",
    "    ya[t] = alpha2 * ya[t-1] + np.random.normal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two time series, $xa$ and $ya$ that have a degree of autocorrelation - let's see what that actually looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two time series\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(xa, label='Time Series X', linestyle='-', marker='o')\n",
    "plt.plot(ya, label='Time Series Y', linestyle='-', marker='x')\n",
    "\n",
    "plt.title('Two Autocorrelated Time Series')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we built these series from an initial random value and at each time step we selected another random value with some added information from the draw before.  But technically these two time series should have very little in common with each other. Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrelation_correlation = sp.stats.pearsonr(xa,ya) \n",
    "autocorrelation_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed that's the case, although there is still some small association (but $p>0.05$).  If we generated enough different time series, we might find that eventually we get 'lucky' and find a set that are correlated.  But for our purposes in the next section, all we need is to keep our eye on that p-value.\n",
    "\n",
    "Thus far we've been using Scipy for our correlation and significance value, but now we're going to need to do something different - we're going to have to account for the loss of degrees of freedom that comes from have time series that are not independent.  Below, we're going to roll our own code to do correlation and significance testing, with the added ability to specify the degrees of freedom and penalize it for autocorrelation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling your own correlation for reduced degrees of freedom\n",
    "\n",
    "The function below will take two time series and the degrees of freedom.  If you thought there was no meaningful autocorrelation, you might make the effective sample size $N_{eff}$ the same as the actual sample size.  But as we'll see, we can pass this function a penalized 'effective' sample size to account for autocorrelation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_correlation_function(x,y,neff):\n",
    "    dof = neff - 2\n",
    "    correlation_matrix = np.corrcoef(x, y)\n",
    "    correlation_value = correlation_matrix[0, 1]\n",
    "    t_statistic = (correlation_value * np.sqrt(dof)) / np.sqrt(1 - correlation_value**2)\n",
    "    p_value = 2 * (1 - sp.stats.t.cdf(abs(t_statistic), df=dof)) \n",
    "    return correlation_value, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we get the same answer as Scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_correlation_function(xa,ya,np.size(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use a calculation of the effective samples size $N_{eff}$ from [Thiebaux and Zwiers 1984](https://journals.ametsoc.org/view/journals/apme/23/5/1520-0450_1984_023_0800_tiaeoe_2_0_co_2.xml), although it has also been described and articulated elsewhere.  The effective sample size is the sample size times the ratio of 1 less the product of the 1st order autocorrelation over 1 plus the product of the 1st order autocorrelations.  The effect here will be that the larger the autocorrelation, the smaller the numerator and the larger the denominator, and therefore the greater penalty:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "N_{\\rm eff} = N \\frac{(1-\\rho_1\\rho_2)}{(1+\\rho_1\\rho_2)}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho1 = lagged_autocorrelation(xa)\n",
    "rho2 = lagged_autocorrelation(ya)\n",
    "N = np.size(xa)\n",
    "\n",
    "Neff = round(N * (1 - rho1 * rho2) / (1 + rho1 * rho2))\n",
    "N, Neff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how using this much reduced effective sample size changes the p-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_correlation_function(xa,ya,Neff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
