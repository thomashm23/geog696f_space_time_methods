{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process for sparse spatial data \n",
    "\n",
    "Thus far we've experimented with Gaussian Process regression using a 1 dimension function -- a time series, for our purposes.  But Gaussian Processes can be applied to multiple dimensions and can be used for modeling spatial fields similar to Kriging.  Kriging is a geostatistical tool that models a (spatial) process as a combination of a mean or trend and a random process with (spatial) autocorrelation.  One can see the similarity with how Gaussian Processes and their kernels work!  Gaussian Process regression is a Bayesian process, however, whereas Kriging is usually a frequentist approach that does the modeling of the process directly from the observations.  Kriging uses a [variogram]() to model the spatial structure, which provides the covariance information, while Gaussian Processes use the kernel as the covariance function.  For kriging in Python, you can use [scikit-gstat](https://scikit-gstat.readthedocs.io/en/latest/) or [PyKrige](https://github.com/GeoStat-Framework/PyKrige).\n",
    "\n",
    "Here's we'll see a few examples of using Gaussian Process to reconstruct a complete 2D spatial field from limited observations of that field.  At the end of this notebook is also a tractable (runs in several minutes) of training on a large sample from one field and using that model to predict or recosntruct a different (although related) field from more limited observations.  Be aware that Gaussian Processes are computationally intensive, so fitting models to large spatial datasets may simply not be practical. \n",
    "\n",
    "Let's get our libraries, including Cartopy for mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import CenteredNorm\n",
    "\n",
    "# you can omit the line below if you'd like, but I really don't like the default fonts in Python, so I switch to Helvetica\n",
    "plt.rcParams['font.family'] = 'Helvetica'\n",
    "\n",
    "import cartopy # base Cartopy\n",
    "import cartopy.crs as ccrs # shortcut to the coordinate reference system\n",
    "import cartopy.feature as cfeature # add map features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll get our Gaussian Process modules from scikit-learn again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to create a simulated Gaussian field and then demonstrate how we can make limited observations of the field, train our Gaussian Process model, and then recover a posterior complete field that is very similar to the 'true' field we started with.  This is a somewhat 'too easy' test, as we're creating a 2D Gaussian field and then applying a Gaussian Process to the reconstruction of that field, but this will serve as a useful demonstration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, write a function that will create a smmoth anisotropic (or eliptical) and off-center Gaussian field  based on a set of coordinates in 'X'\n",
    "def make_spatial_field(X):\n",
    "    return np.exp(-((X[:, 0] - 0.7) ** 2 / 0.1 + (X[:, 1] - 0.5) ** 2 / 0.02))\n",
    "\n",
    "# create a 2D grid of points X1 [0, 1] and X2[ 0, 1] with 100 coordinates in each of those dimensions - we'll define our 'true' field on this simple 1x1 grid\n",
    "x1 = np.linspace(0, 1, 100) # 100 coordinates between 0 and 1 - this makes it easy to get random locations using the uniform distribution\n",
    "x2 = np.linspace(0, 1, 100) # 100 coordinates\n",
    "X1, X2 = np.meshgrid(x1, x2) # X1 and X2 are now both 100 x 100 arrays describing the paired coordinates at each location in the regular 2D grid\n",
    "X_grid = np.vstack([X1.ravel(), X2.ravel()]).T # X_grid is now a long 2 column strung-out-vector of all the X1,X2 coordinate pairs, so 10000 x 2 columns\n",
    "\n",
    "# Generate the true function values for that entire 10,000 member grid\n",
    "y_true_grid = make_spatial_field(X_grid) # create an exponential function based on coordinate pairs, this returns 2 columns for simplicity although there is probably a better way to do this\n",
    "y_true_grid_reshaped = y_true_grid.reshape(X1.shape) # now reshape that strung-out-vector back into a complete 2D grid\n",
    "\n",
    "# sample randomly from the true field to mimic sparse observations of that field without noise\n",
    "n_observations = 50\n",
    "np.random.seed(1999) # for reproducibility, but try different seed numbers here to see a range of behaviors \n",
    "X_train = np.random.rand(n_observations, 2)  # Random 2D locations for observations using a draw from a uniform distribution [0,1] as it matches the artificial coordinate system\n",
    "\n",
    "# generate our simulated data now for just the training points using the function again -  probably a better way to do this from the true_grid, but this is easy and works\n",
    "y_train = make_spatial_field(X_train)\n",
    "\n",
    "# set a constant and an RBF kernel - the RBF kernel now has 2 different initial length scales because we know the field is anisotropic - but what if we didn't? \n",
    "# from the scikit-learn documentation: 'If [length scale is] an array, an anisotropic kernel is used where each dimension of l defines the length-scale of the respective feature dimension.'\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=[0.1, 0.5], length_scale_bounds=(1e-3, 1e3)) # make the starting value longer in X1 than X2 based on our knowledge of the system\n",
    "\n",
    "# create the Gaussian Process regressor object\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
    "\n",
    "# fit the gp model on the limited 'observed' data\n",
    "gp.fit(X_train, y_train)\n",
    "\n",
    "# ... then predict the values of the field at all the grid point values (10,000 of them) and return the posterior and the uncertainty\n",
    "y_pred, sigma = gp.predict(X_grid, return_std=True)\n",
    "\n",
    "# reshape posterior (the prediction) back to the grid for plotting\n",
    "y_pred_grid = y_pred.reshape(X1.shape)\n",
    "sigma_grid = sigma.reshape(X1.shape)\n",
    "\n",
    "# calculate the difference between the true field and the posterior field\n",
    "difference_grid = y_pred_grid - y_true_grid_reshaped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make some plots of the 'true' (simulated) field, the 'observations' of that field, the posterior or reconstructed field, and the difference between true and reconstructed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a common vmin and vmax so we can set consistent levels and colorbars\n",
    "# vmin = min(y_true_grid.min(), y_train.min(), y_pred.min()) # this automatically sets vmin\n",
    "# vmax = max(y_true_grid.max(), y_train.max(), y_pred.max()) # this automatically sets vmax\n",
    "vmin = -0.1 # or manually set vmin based on first seeing the minimum values across 3 subplots\n",
    "vmax = 1.1  # or manually set vmax based on first seeing the maximum values across 3 subplots\n",
    "norm = colors.Normalize(vmin=vmin, vmax=vmax, clip=True) # normalize the colormap for consistency\n",
    "levels = np.linspace(vmin, vmax, 13) # set levels based on vmin and vmax so that colorbars are uniform across 3 plots\n",
    "\n",
    "# plot the True Field, the random observations, the Posterior Field, and their difference\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "# true field\n",
    "plt.subplot(1, 4, 1)\n",
    "contour1 = plt.contourf(X1, X2, y_true_grid_reshaped, cmap='viridis',levels=levels,vmin=vmin, vmax=vmax,norm=norm)\n",
    "plt.colorbar(contour1)\n",
    "plt.title('True Field')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "\n",
    "# observations, colored appropriately using y_train to get the colors\n",
    "plt.subplot(1, 4, 2)\n",
    "scatter = plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', edgecolor='k',vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(scatter)\n",
    "plt.title('Random Samples')\n",
    "plt.xlabel('x1')\n",
    "\n",
    "# the posterior field\n",
    "plt.subplot(1, 4, 3)\n",
    "contour2 = plt.contourf(X1, X2, y_pred_grid, cmap='viridis',levels=levels,vmin=vmin, vmax=vmax,norm=norm)\n",
    "plt.colorbar(contour2)\n",
    "plt.title('Posterior Field')\n",
    "plt.xlabel('x1')\n",
    "\n",
    "# plot the difference between the true field and the posterior field\n",
    "plt.subplot(1, 4, 4)\n",
    "contour3 = plt.contourf(X1, X2, difference_grid, cmap='RdBu_r', levels=21, norm=CenteredNorm()) # center the colorbar at zero (white)\n",
    "plt.colorbar(contour3)\n",
    "plt.title('Difference (Posterior - True))')\n",
    "plt.xlabel('x1')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A real SST field\n",
    "\n",
    "Let's use the same approach on a real SST field.  We'll once again get our Hadley ISST data using xarray and create a DJF average for the tropical Pacific.  Unlike the previous EOF notebook, we'll restrict outselves to just the tropical domain (20N to 20S) to make the data a bit more computationally tractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open our netcdf file into a DataSet and take a look:\n",
    "ds = xr.open_dataset('HadISST_sst.nc') \n",
    "\n",
    "# pull out sst only as a DataArray\n",
    "sst = ds.sst\n",
    "\n",
    "# deal with missing and negative values in SST\n",
    "sst = sst.where(sst >= -4, np.nan) \n",
    "nan_mask = sst.isnull().any(dim='time')\n",
    "sst = sst.where(~nan_mask, np.nan)\n",
    "\n",
    "# change the longitudes from -180,180 to 0,360 \n",
    "sst = sst.assign_coords(longitude=(sst.longitude % 360))\n",
    "sst = sst.sortby(sst.longitude)\n",
    "\n",
    "# latitude and longitude bounds for tropical and Pacific SST\n",
    "min_lat = -20\n",
    "max_lat = 20\n",
    "min_lon = 120\n",
    "max_lon = 280\n",
    "\n",
    "# select a slice and overwrite sst with the output and then take a look\n",
    "sst = sst.sel(longitude=slice(min_lon,max_lon),latitude=slice(max_lat,min_lat))\n",
    "\n",
    "# calculate the monthly mean at each point and remove it from each point to get monthly anomalies (SSTA)\n",
    "sst_clm = sst.groupby('time.month').mean(dim='time')\n",
    "sst_anom = sst.groupby('time.month') - sst_clm\n",
    "\n",
    "# Calculate the 3-month rolling mean and keep a winter (DJF) mean value\n",
    "sst_ = sst_anom.rolling(time=3).mean(skipna=True)\n",
    "djf = sst_[sst_.time.dt.month == 2]\n",
    "djf = djf.drop_isel(time=0) # remove incomplete year \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull out a single year -- 1998.  December 1997 through February 1998 was the peak of one of the strongest El Nino events on record.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djf_1998 = djf.sel(time='1998').squeeze() # squeeze out the singleton dimension here\n",
    "djf_1998\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now setup our Gaussian Process regression for DJF 1997-1998 in the tropical Pacific.  We'll first pull out the latitude and longitude values and the SST values to build our training set.  We use `.meshgrid` from Numpy to create the corresponding longitude and latitudes at each grid point (you can see lon_grid and lat_grid are both 40 by 160, each containing the longitude or latitude for each grid point in 2D space) and then create a strung-out-vector of 2 columns to pass to the Gaussian Process regressor (note again that `.fit` is going to expect the data as 'n_samples, n_features' according to the scikit-learn documentation, so each column here is a feature - a grid point location of longitude and latitude).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the data for the Gaussian Process regression\n",
    "latitudes = djf_1998['latitude'].values\n",
    "longitudes = djf_1998['longitude'].values\n",
    "sst_values = djf_1998.values.flatten()\n",
    "\n",
    "# create a full set of paired longitude and latitude coordinates for each grid point in our domain\n",
    "lon_grid, lat_grid = np.meshgrid(longitudes, latitudes)\n",
    "coords = np.column_stack([lon_grid.ravel(), lat_grid.ravel()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get a small random sample from the SST field that will serve as our observations.  We will use `np.random.choice` to select index values of n_samples, then apply that random index to the coordinate training (e.g. the longitude and latitude coordinates) and the value training sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get a random set of observations across the field\n",
    "n_samples = 100 # the quality of the single year GP kriging will be very sensitive to this\n",
    "indices = np.random.choice(len(coords), size=n_samples, replace=False)\n",
    "X_train = coords[indices]  # these are the coordinates (lon, lat) of our observations \n",
    "y_train = sst_values[indices]  # and these are corresponding values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll once again define an anisotropic RBF kernel with two starting length scales - since this is the tropical Pacific, we might expect the longitude length scale should be more stretched or extended than the latitude one.  We'll also limit our training dataset now for the fit to only locations that have a valid (non-NaN) SST value (e.g. we will not use points that happened to come from contients, as the fit will fail).  We'll `.fit` our model using the limited observations only.  \n",
    "\n",
    "We'll then use that model fit to predict SST values at all the points in the field (note that this will predict SST values for the continents - I could and should probably mask and make these NaN, but for now I'll  just cover them with continents).  We'll then turn our posterior reconstruction and uncertainty into DataArrays again (they come out of the Gaussian Process fit and predict as Numpy arrays):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the kernel and create the Gussian Process object\n",
    "kernel = C(1.0) * RBF(length_scale=[100,20]) # units are degrees of longitude and latitude, so use those to guide length scale initialization - could also add e.g. length_scale_bounds=(1.0, 1000.0) \n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=50)\n",
    "\n",
    "# remove any NaN values from the training data that come from points placed on continents - maybe not the best way to do this, but the easiest for now\n",
    "valid_indices = ~np.isnan(y_train)\n",
    "X_train_valid = X_train[valid_indices]\n",
    "y_train_valid = y_train[valid_indices]\n",
    "\n",
    "# Fit the Gaussian Process model\n",
    "gp.fit(X_train_valid, y_train_valid)\n",
    "\n",
    "# Prepare the full grid for predictions\n",
    "X_pred = coords  # Coordinates of all grid points (lon, lat)\n",
    "\n",
    "# Make predictions on the full field\n",
    "y_pred, y_std = gp.predict(X_pred, return_std=True)\n",
    "\n",
    "# Reshape the predicted values back to the original shape\n",
    "sst_pred = y_pred.reshape(djf_1998.shape)\n",
    "sst_std = y_std.reshape(djf_1998.shape)\n",
    "\n",
    "# Convert the predictions back to an xarray DataArray for easier handling\n",
    "sst_pred_da = xr.DataArray(sst_pred, coords=djf_1998.coords, dims=djf_1998.dims)\n",
    "sst_std_da = xr.DataArray(sst_std, coords=djf_1998.coords, dims=djf_1998.dims)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some plots again - once again we'll look at the known DJF 1997-1998 SST anomaly field, where our 'observations' occur, the posterior reconstructed field, and the difference or error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = sst_pred_da - djf_1998\n",
    "\n",
    "# define the projection for all subplots - we'll use Miller centered on the tropical Pacific \n",
    "projection = ccrs.Miller(central_longitude=260)\n",
    "\n",
    "# create a figure with 4 subplots, using the same projection for each\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 6), subplot_kw={'projection': projection}, constrained_layout=True)\n",
    "axs = axs.flatten() # flatten the axes handle for simpler referencing\n",
    "\n",
    "# plot the true field\n",
    "im1 = axs[0].pcolormesh(djf_1998['longitude'], djf_1998['latitude'], djf_1998.squeeze(), cmap='RdBu_r', shading='auto', vmin=-4, vmax=4, transform=ccrs.PlateCarree()) # squeeze the singleton dimension for plotting\n",
    "axs[0].set_title('True SST Field (DJF 1997-1998)')\n",
    "axs[0].coastlines()\n",
    "axs[0].add_feature(cfeature.LAND, color='black')\n",
    "axs[0].set_extent([min_lon, max_lon, min_lat, max_lat], crs=ccrs.PlateCarree())\n",
    "plt.colorbar(im1, ax=axs[0], orientation='horizontal', label='SST (C)')\n",
    "\n",
    "# plot the observations \n",
    "sc = axs[1].scatter(X_train_valid[:, 0], X_train_valid[:, 1],c=y_train_valid,cmap='RdBu_r',edgecolor='k',s=50,marker='o', vmin=-4, vmax=4, transform=ccrs.PlateCarree())\n",
    "axs[1].set_title('Observed Points')\n",
    "axs[1].coastlines()\n",
    "axs[1].add_feature(cfeature.LAND, color='black')\n",
    "axs[1].set_extent([min_lon, max_lon, min_lat, max_lat], crs=ccrs.PlateCarree())\n",
    "plt.colorbar(sc, ax=axs[1], orientation='horizontal', label='SST (C)')\n",
    "\n",
    "# plot the reconstructed field\n",
    "im2 = axs[2].pcolormesh(sst_pred_da['longitude'], sst_pred_da['latitude'], sst_pred_da.squeeze(), cmap='RdBu_r', shading='auto',  vmin=-4, vmax=4, transform=ccrs.PlateCarree())\n",
    "axs[2].set_title('Reconstructed SST Field')\n",
    "axs[2].coastlines()\n",
    "axs[2].add_feature(cfeature.LAND, color='black')\n",
    "axs[2].set_extent([min_lon, max_lon, min_lat, max_lat], crs=ccrs.PlateCarree())\n",
    "plt.colorbar(im2, ax=axs[2], orientation='horizontal', label='SST (C)')\n",
    "\n",
    "# plot the difference between the true and reconstructed fields\n",
    "im3 = axs[3].pcolormesh(difference['longitude'], difference['latitude'], difference.squeeze(), cmap='RdBu_r', shading='auto', norm=CenteredNorm(), transform=ccrs.PlateCarree())\n",
    "axs[3].set_title('Difference (Reconstructed - True)')\n",
    "axs[3].coastlines()\n",
    "axs[3].add_feature(cfeature.LAND, color='black')\n",
    "axs[3].set_extent([min_lon, max_lon, min_lat, max_lat], crs=ccrs.PlateCarree())\n",
    "plt.colorbar(im3, ax=axs[3], orientation='horizontal', label='Difference (C)')\n",
    "\n",
    "for ax in axs:\n",
    "    gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    gl.xlabel_style = {'size': 8, 'color': 'black'}\n",
    "    gl.ylabel_style = {'size': 8, 'color': 'black'}\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior reconstructed field generally captures the assymetric warming pattern seen in the full SST field.  However, it is too 'spotty' and too warm in regions with no observations in the warmest parts of if the eastern equatorial Pacific.  The length scale hyperparameters of the posterior are also quite short:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.kernel_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "1. Experiment with the settings for the RBF kernel - above I include two dimensions, again with a longer initial length scale for the zonal (longitude) dimension vs the medidional (latitude) dimension.  What if I don't do that? How does that effect the shape of the SST reconstruction?\n",
    "2. Incrementally increase the number of observations - how does this affect the posterior reconstructed field? \n",
    "3. Similarly, experiment with using some _different_ classes of kernels - you can see the options [here](https://scikit-learn.org/1.5/modules/gaussian_process.html#kernels-for-gaussian-processes).  For instance, what happens if you use the Matern kernel instead of the RBF? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section below is experimental.  In theory, we should be able to get a better posterior reconstruction if the model is fit on a larger set of data.  Unfortunately, Gaussian Processes are so computationally expensive that training the model on e.g. the full field from a different ENSO year (let alone the full dataset) would take an extremely long time to complete (and might even crash your kernel).  Here is a compromise, where we train on a large subset (1000 or greater points) and then predict on a limited set of points from 1998 again.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on a subset of a different year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract just the 1983 data -- another El Nino winter! - from the dataset\n",
    "djf_1983 = djf.sel(time='1983')\n",
    "\n",
    "# prepare coordinates and values for training\n",
    "latitudes_1983 = djf_1983['latitude'].values\n",
    "longitudes_1983 = djf_1983['longitude'].values\n",
    "sst_values_1983 = djf_1983.values.flatten()\n",
    "\n",
    "# create a full set of paired longitude and latitude coordinates for each grid point in 1983\n",
    "lon_grid_1983, lat_grid_1983 = np.meshgrid(longitudes_1983, latitudes_1983)\n",
    "coords_1983 = np.column_stack([lon_grid_1983.ravel(), lat_grid_1983.ravel()])\n",
    "\n",
    "# get a larger subsample for training with 1983\n",
    "n_samples_1983 = 3000  # tried 1000 and 2000; 5000 still takes a very long time! \n",
    "indices_1983 = np.random.choice(len(coords_1983), size=n_samples_1983, replace=False)\n",
    "X_train_1983 = coords_1983[indices_1983]  # Coordinates (lon, lat) of observations\n",
    "y_train_1983 = sst_values_1983[indices_1983]  # Corresponding SST values\n",
    "\n",
    "# remove NaN values from the training data due to selecting land points\n",
    "valid_indices_1983 = ~np.isnan(y_train_1983)\n",
    "X_train_1983_valid = X_train_1983[valid_indices_1983]\n",
    "y_train_1983_valid = y_train_1983[valid_indices_1983]\n",
    "\n",
    "# define the kernel and create the Gaussian Process object\n",
    "kernel = C(1.0) * RBF(length_scale=[100, 20],length_scale_bounds=(2.0, 1000.0)) # forcing the length scale to be longer helps improve the reconstruction, but forcing it to be very long gives you a very smooth field\n",
    "gp0 = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
    "\n",
    "# fit the GP model using the (valid) training data from 1983\n",
    "gp0.fit(X_train_1983_valid, y_train_1983_valid)\n",
    "\n",
    "# extract 1998 data for prediction and reconstruction\n",
    "djf_1998 = djf.sel(time='1998')\n",
    "latitudes_1998 = djf_1998['latitude'].values\n",
    "longitudes_1998 = djf_1998['longitude'].values\n",
    "sst_values_1998 = djf_1998.values.flatten()\n",
    "\n",
    "# create the full grid of coordinates for 1998\n",
    "lon_grid_1998, lat_grid_1998 = np.meshgrid(longitudes_1998, latitudes_1998)\n",
    "coords_1998 = np.column_stack([lon_grid_1998.ravel(), lat_grid_1998.ravel()])\n",
    "\n",
    "# select the limited random set of observations for 1998\n",
    "n_samples_1998 = 100  \n",
    "indices_1998 = np.random.choice(len(coords_1998), size=n_samples_1998, replace=False)\n",
    "X_train_1998 = coords_1998[indices_1998]\n",
    "y_train_1998 = sst_values_1998[indices_1998]\n",
    "\n",
    "# remove NaN values from the limited observations for 1998\n",
    "valid_indices_1998 = ~np.isnan(y_train_1998)\n",
    "X_train_1998_valid = X_train_1998[valid_indices_1998]\n",
    "y_train_1998_valid = y_train_1998[valid_indices_1998]\n",
    "\n",
    "# use the pre-trained GP model from 1983 to predict the full 1998 field from the limited observations\n",
    "X_pred_1998 = coords_1998  # Full grid of coordinates for 1998\n",
    "\n",
    "# Make predictions on the full field for 1998 using the GP model trained on 1983 data\n",
    "y_pred_1998, y_std_1998 = gp0.predict(X_pred_1998, return_std=True)\n",
    "\n",
    "# Reshape the predicted values back to the original shape of the 1998 data\n",
    "sst_pred_1998 = y_pred_1998.reshape(djf_1998.shape)\n",
    "sst_std_1998 = y_std_1998.reshape(djf_1998.shape)\n",
    "\n",
    "# Convert the predictions back to an xarray DataArray for easier handling\n",
    "sst_pred_da_1998 = xr.DataArray(sst_pred_1998, coords=djf_1998.coords, dims=djf_1998.dims)\n",
    "sst_std_da_1998 = xr.DataArray(sst_std_1998, coords=djf_1998.coords, dims=djf_1998.dims)\n",
    "\n",
    "# for 1000 training points, time for this code block is between 15 seconds and 1 minute ?!\n",
    "# for 2000 training points, time for this code block is 50 seconds to ~5 minutes\n",
    "# for 3000 training points, time for this code block is >2 minutes to ~6 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the posterior hyperparameters look any better? Without additional constraints, the posterior hyperparameters appear to find a lower bound - why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp0.kernel_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting code for the model fitting and reconstruction above - even with a larger training, the lack of observations in the warm eastern equatorial Pacific still results in isolated area of too warm reconstructed temperatures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = sst_pred_da_1998 - djf_1998\n",
    "\n",
    "# same  projection for all subplots\n",
    "projection = ccrs.Miller(central_longitude=260)\n",
    "\n",
    "# this seems to work better than the old way of doing it? \n",
    "land_feature = cfeature.NaturalEarthFeature('physical', 'land', '50m', edgecolor='none', facecolor='black')\n",
    "\n",
    "# create a figure with 4 subplots, using the same projection for each\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 6), subplot_kw={'projection': projection}, constrained_layout=True)\n",
    "axs = axs.flatten()  # Flatten the axes handle for simpler reference\n",
    "\n",
    "# the known field\n",
    "pc1 = axs[0].pcolormesh(djf_1998['longitude'], djf_1998['latitude'], djf_1998.squeeze(), cmap='RdBu_r', shading='auto', vmin=-4, vmax=4, transform=ccrs.PlateCarree())\n",
    "axs[0].set_title('True SST Field (DJF 1997-1998)')\n",
    "axs[0].coastlines()\n",
    "axs[0].add_feature(land_feature)\n",
    "axs[0].set_extent([min_lon, max_lon, min_lat, max_lat], crs=ccrs.PlateCarree())\n",
    "plt.colorbar(pc1, ax=axs[0], orientation='horizontal', label='SST (°C)')\n",
    "\n",
    "# observations\n",
    "sc = axs[1].scatter(X_train_1998_valid[:, 0], X_train_1998_valid[:, 1], c=y_train_1998_valid, cmap='RdBu_r',edgecolor='k', s=50, marker='o', vmin=-4, vmax=4, transform=ccrs.PlateCarree())\n",
    "axs[1].set_title('Observed Points')\n",
    "axs[1].coastlines()\n",
    "axs[1].add_feature(land_feature)\n",
    "axs[1].set_extent([min_lon, max_lon, min_lat, max_lat], crs=ccrs.PlateCarree())\n",
    "plt.colorbar(sc, ax=axs[1], orientation='horizontal', label='SST (°C)')\n",
    "\n",
    "#  the reconstructed field\n",
    "pc2 = axs[2].pcolormesh(sst_pred_da_1998['longitude'], sst_pred_da_1998['latitude'], sst_pred_da_1998.squeeze(),cmap='RdBu_r', shading='auto', vmin=-4, vmax=4, transform=ccrs.PlateCarree())\n",
    "axs[2].set_title('Reconstructed SST Field')\n",
    "axs[2].coastlines()\n",
    "axs[2].add_feature(land_feature)\n",
    "axs[2].set_extent([min_lon, max_lon, min_lat, max_lat], crs=ccrs.PlateCarree())\n",
    "plt.colorbar(pc2, ax=axs[2], orientation='horizontal', label='SST (C)')\n",
    "\n",
    "# Plot the difference between the true and reconstructed fields\n",
    "pc3 = axs[3].pcolormesh(difference['longitude'], difference['latitude'], difference.squeeze(),cmap='RdBu_r', shading='auto', norm=CenteredNorm(), transform=ccrs.PlateCarree())\n",
    "axs[3].set_title('Difference (Reconstructed - True)')\n",
    "axs[3].coastlines()\n",
    "axs[3].add_feature(land_feature)\n",
    "axs[3].set_extent([min_lon, max_lon, min_lat, max_lat], crs=ccrs.PlateCarree())\n",
    "plt.colorbar(pc3, ax=axs[3], orientation='horizontal', label='Difference (C)')\n",
    "\n",
    "for ax in axs:\n",
    "    gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    gl.xlabel_style = {'size': 8, 'color': 'black'}\n",
    "    gl.ylabel_style = {'size': 8, 'color': 'black'}\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geog696fpython311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
