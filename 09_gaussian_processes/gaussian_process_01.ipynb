{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Gaussian Processes\n",
    "\n",
    "This notebook is intended to complement the introductory lecture on Gaussian Processes.  We'll first look at Gaussian Processes in the context of the interpolation of missing data we did in the previous notebook, then move onto demonstrating a 1D application of Gaussian Process regression in context of various sparse datasets.\n",
    "\n",
    "Let's get our libraries first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np \n",
    "import scipy as sp\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# switch font default to Helvetica\n",
    "plt.rcParams['font.family'] = 'Helvetica'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, let's create a simulated time series as a sine function of 64 time steps ('years).  We'l also remove some parts of the time series again and plot the function and the function with missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use 64 time points \n",
    "n_samples = 64\n",
    "\n",
    "# Create our time vector - e.g. 64 years long\n",
    "t = np.arange(1, n_samples+1)\n",
    "\n",
    "# Create a sine wave with frequency set to the variable cycles_per_time \n",
    "cycles_per_time = 2\n",
    "cycle_amplitude = 3 \n",
    "u = 2 * np.pi * (cycles_per_time/128) * t\n",
    "\n",
    "# here is our simulated time seres\n",
    "st = cycle_amplitude * np.sin(u)\n",
    "\n",
    "# create a copy using Numpy of st that we can remove some data from\n",
    "stm = st.copy()\n",
    "\n",
    "# remove two chunks of data from stm and replace with NaNs\n",
    "stm[12:20] = np.nan\n",
    "stm[30:37] = np.nan    \n",
    "    \n",
    "# Plot the original time series and the stm series with NaN values\n",
    "plt.figure()\n",
    "plt.plot(t, st, 'r-')\n",
    "plt.plot(t, stm, 'ko')\n",
    "plt.xlabel('Years')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various libraries available in the Python ecosystem for Gaussian Processes (regression and classification).  Here we'll use scikit-learn again, which has a broad range of functionality for [Gaussian Processes](https://scikit-learn.org/1.5/modules/gaussian_process.html) and specifically the [`GaussianProcessRegressor`](https://scikit-learn.org/dev/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html).   We'll also need to get kernels (or covariance functions) from `sklearn.gaussian_process.kernels`.  Here we'll get the Constant kernel and the Radial Basis Function ([RBF](https://scikit-learn.org/1.5/modules/generated/sklearn.gaussian_process.kernels.RBF.html)) kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to apply the `GaussianProcessRegressor` to our missing data function above.  The first thing we do is find the valid (not-missing) element in the function to train the Gaussian Process.  We then declare a very simple kernel - a RBF (discussed in lecture) with the default starting length scale and the default length scale range for the model fit to consider (default is: length_scale_bounds=(1e-05, 100000.0)).  \n",
    "\n",
    "We then call the GaussianProcessRegressor to create `gp`, the object that will be used to fit and then predict from the model.  We'll use `gp.fit` to fit the model with our x-coordinate ('t') and y-value (stm).  We'll use `gp.predict` then to get the form of the Gaussian Process at all the values of the x-coordinate (even where the y-values are missing) and consider this in the same way we did the interpolation in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the non-missing entries in the variable to use for training\n",
    "valid = ~np.isnan(stm)\n",
    "\n",
    "# Define the kernel (RBF is common for smooth functions)\n",
    "# kernel = RBF(length_scale=1.0)\n",
    "kernel = ConstantKernel(1.0) * RBF(length_scale=1.0) # can define with constant here, but since data are centered doesn't have much effect\n",
    "\n",
    "\n",
    "# Create the Gaussian Process model\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=20)\n",
    "\n",
    "# Fit the model on non-NaN data\n",
    "gp.fit(t[valid].reshape(-1, 1), stm[valid])\n",
    "\n",
    "# Predict over all data points, including NaNs\n",
    "stm_gp, sigma = gp.predict(t.reshape(-1, 1), return_std=True)\n",
    "\n",
    "# Plot the simulated noise-free time series\n",
    "plt.figure()\n",
    "plt.plot(t, st, 'r')\n",
    "plt.plot(t, stm, 'ko')\n",
    "plt.plot(t, stm_gp, 'b--')\n",
    "plt.xlabel('Years')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple Gaussian Process regression find an excellent fit to the underlying function - likely because we have lots of correlated observations. \n",
    "\n",
    "What about if we have very few observations of the underlying function?  Interpolation of most sorts would probably struggle if we only had 4 or 5 observations of the 64 time points, but as we'll see, Gaussian Process regression can find a likely function _and_ uncertainty for that functional fit, even when observations are sparse.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy using Numpy of st that we can remove some data from\n",
    "stm = st.copy()\n",
    "\n",
    "# Define the number of training points or observations to select\n",
    "n_samples = 4\n",
    "\n",
    "# Randomly select some index values to subsample our signal\n",
    "random_indices = np.random.choice(len(stm), size=n_samples, replace=False)\n",
    "\n",
    "# Extract the selected points from `stm` and `t` into new training variables\n",
    "stm_train = stm[random_indices]\n",
    "t_train = t[random_indices]\n",
    "\n",
    "# Create the Gaussian Process model\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=20)\n",
    "\n",
    "# Fit the model on non-NaN data\n",
    "gp.fit(t_train.reshape(-1, 1), stm_train)\n",
    "\n",
    "# Predict over all data points in our time vector, t\n",
    "stm_gp_sparse, stm_sigma_sparse = gp.predict(t.reshape(-1, 1), return_std=True)\n",
    "\n",
    "# Plot the simulated noise-free time series\n",
    "plt.figure()\n",
    "plt.plot(t, st, 'r--',label='True Function')\n",
    "plt.plot(t_train, stm_train, 'ko',label='Observations')\n",
    "plt.plot(t, stm_gp_sparse, 'b-',label='Mean Prediction')\n",
    "plt.fill_between(t, stm_gp_sparse-(2*stm_sigma_sparse), stm_gp_sparse+(2*stm_sigma_sparse), color='lightblue', alpha=0.5,label='Prediction uncertainties (2Ïƒ)')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Years')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the posterior solution does a good job when the underlying function is well-bounded by the few data points.  It may do OK in some instances away from those observation or between observation and the time limits, but in other cases there is a very large uncertainty (that may -- but may not -- contain the underlying function).  Because we are randomly selecting observations to retain, you can run the above code block many times and look at the various posterior solutions and the uncertainty range as a function of the observations in each random draw.\n",
    "\n",
    "We can get the kernel that the Gaussian Process finally selected by passing the following command to the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.kernel_ # this allows us to see the hyper-parameters (variance from the constant, and for RBF the length scale) of the GP fit to the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How might you interpet that length scale in light of the underlying true function and the limited observations?  See how much this changes for different sets of observations and posterior fits to the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we assume that we know the observations y-value without error.  But that is seldom the case in the real world!  We can add error to our observations and also tell the Gaussian Process to expect some uncertainty by specifying a value for `alpha` -- in this case, since we can specify the noise in the observations we can also pass it to the GaussianProcessRegressor exactly (as the square of the standard deviation of the noise component).  In the real world, you'd need to have some prior information or a reasonable estimate of the noise to give to the GaussianProcessRegressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy using Numpy of st that we can remove some data from\n",
    "stm = st.copy()\n",
    "\n",
    "# Define the number of training points or observations to select\n",
    "n_samples = 4\n",
    "\n",
    "# Randomly select some index values to subsample our signal\n",
    "random_indices = np.random.choice(len(stm), size=n_samples, replace=False)\n",
    "\n",
    "# Extract the selected points from `stm` and `t` into new training variables\n",
    "stm_train = stm[random_indices]\n",
    "noise_std = 0.50\n",
    "stm_train_noisy = stm_train + np.random.normal(loc=0.0, scale=noise_std, size=stm_train.shape)\n",
    "t_train = t[random_indices]\n",
    "\n",
    "# Create the Gaussian Process model\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=noise_std**2, n_restarts_optimizer=20)\n",
    "\n",
    "# Fit the model on non-NaN data\n",
    "gp.fit(t_train.reshape(-1, 1), stm_train_noisy)\n",
    "\n",
    "# Predict over all data points in our time vector, t\n",
    "stm_gp_sparse, stm_sigma_sparse = gp.predict(t.reshape(-1, 1), return_std=True)\n",
    "\n",
    "# Plot the simulated noise-free time series\n",
    "plt.figure()\n",
    "plt.plot(t, st, 'r--',label='True Function')\n",
    "plt.errorbar(t_train, stm_train_noisy, noise_std, linestyle='none',color='k',marker='o',label='Noisy Observations')\n",
    "plt.plot(t, stm_gp_sparse, 'b-',label='Mean Prediction')\n",
    "plt.fill_between(t, stm_gp_sparse-(2*stm_sigma_sparse), stm_gp_sparse+(2*stm_sigma_sparse), color='lightblue', alpha=0.5,label='Prediction uncertainties (2Ïƒ)')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Years')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that allowing of uncertainty in the observations allows the posterior solution to 'relax' around the observations and not fit them as tightly. \n",
    "\n",
    "We can see if this affects the kernel hyperparameters as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.kernel_ # compare to various runs of the better-populated curve above where only a few values were missing - what do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last example.  What if we observe 64 time steps (no missing data) but the data are very noisy?  How does the Gaussian Process regressor do at fitting the true underlying function? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy using Numpy of st that we can remove some data from\n",
    "stm = st.copy()\n",
    "\n",
    "# Extract the selected points from `stm` and `t` into new training variables\n",
    "stm_train = stm \n",
    "noise_std = 0.75\n",
    "stm_train_noisy = stm_train + np.random.normal(loc=0.0, scale=noise_std, size=stm_train.shape)\n",
    "t_train = t\n",
    "\n",
    "# Create the Gaussian Process model\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=noise_std**2, n_restarts_optimizer=20)\n",
    "\n",
    "# Fit the model on non-NaN data\n",
    "gp.fit(t_train.reshape(-1, 1), stm_train_noisy)\n",
    "\n",
    "# Predict over all data points in our time vector, t\n",
    "stm_gp_sparse, stm_sigma_sparse = gp.predict(t.reshape(-1, 1), return_std=True)\n",
    "\n",
    "# Plot the simulated noise-free time series\n",
    "plt.figure()\n",
    "plt.plot(t, st, 'r--',label='True Function')\n",
    "plt.errorbar(t_train, stm_train_noisy, noise_std, linestyle='none',color='k',marker='o',label='Noisy Observations')\n",
    "plt.plot(t, stm_gp_sparse, 'b-',label='Mean Prediction')\n",
    "plt.fill_between(t, stm_gp_sparse-(2*stm_sigma_sparse), stm_gp_sparse+(2*stm_sigma_sparse), color='lightblue', alpha=0.5,label='Prediction uncertainties (2Ïƒ)')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Years')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad!  We can look at the posterior kernel hyperparameter again here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.kernel_ # highly variable, but in the ~9 to ~17 time step range, depending on the noise structure in each random draw - what do you think this means? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing - let's consider the autocorrelation function of the noisy sine wave function.  How does the length scale parameter seem to relate to this autocorrelation structure? What do you make of the similarity or difference in the autocorrelation lags and length scale? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "acf_values = acf(stm_train_noisy, nlags=32, fft=True)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.stem(range(33), acf_values)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. There are a lot of options -- especially regarding [kernel design](https://scikit-learn.org/dev/api/sklearn.gaussian_process.html#module-sklearn.gaussian_process.kernels) -- for the [Gaussian Process Regressor](https://scikit-learn.org/dev/api/sklearn.gaussian_process.html).  Review the different types of kernels and see if a different choice of kernel, or a different starting value for the hyperparameter, makes a difference in the quality of the model fits above. \n",
    "2. What happens if you specify a different initial length scale for the RBF kernel?  Does it make a difference at all in the fit or posterior hyperparameters? \n",
    "3. Experiment with setting the `alpha` values in the noise examples above intentionally to the 'wrong' value - how does this affect the posterior solution? \n",
    "4. Go through the CO2 fitting exercise using Gaussian Process Regression [here](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_co2.html).  In addition to using a dataset we've used before, the design of the kernel shows how important and powerful this part of the procedure can be. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geog696fpython311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
