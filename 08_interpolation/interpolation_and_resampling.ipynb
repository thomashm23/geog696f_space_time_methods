{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation, imputation, and resampling\n",
    "\n",
    "This notebook will take us through a basic overview of approaches to dealing with missing data (interpolation and imputation) and resampling (usually for dealing with age models for time series).  As with most things in Python, there are multiple ways to approach these issues and functions or classes spread across multiple libraries.  We'll look at a few of the primary methods and libraries here.\n",
    "\n",
    "First, let's get our libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np \n",
    "import scipy as sp\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# switch font default to Helvetica\n",
    "plt.rcParams['font.family'] = 'Helvetica'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to create a simulated time series again (as before) using a sine wave.  It is important to note, however, that different series and different patterns of missing data may be more ameniable to a set of different approaches to interpolation (or imputation).  Here we'll see ones that work well, and others that are less successful in reproducing the underlying function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use 64 time points \n",
    "n_samples = 64\n",
    "\n",
    "# Create our time vector - e.g. 64 years long\n",
    "t = np.arange(1, n_samples+1)\n",
    "\n",
    "# Create a sine wave with frequency set to the variable cycles_per_time \n",
    "cycles_per_time = 2\n",
    "cycle_amplitude = 3 \n",
    "u = 2 * np.pi * (cycles_per_time/128) * t\n",
    "\n",
    "# here is our simulated time seres\n",
    "st = cycle_amplitude * np.sin(u)\n",
    "    \n",
    "# Plot the simulated noise-free time series\n",
    "plt.figure()\n",
    "plt.plot(t, st, 'r')\n",
    "plt.xlabel('Years')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now remove some of the data from the series above and attempt to interpolate or impute it, comparing the result to the known underlying function we created.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy using Numpy of st that we can remove some data from\n",
    "stm = st.copy()\n",
    "\n",
    "# remove two chunks of data from stm and replace with NaNs\n",
    "stm[12:20] = np.nan\n",
    "stm[30:37] = np.nan\n",
    "\n",
    "# Plot the original time series and the stm series with NaN values\n",
    "plt.figure()\n",
    "plt.plot(t, st, 'r-')\n",
    "plt.plot(t, stm, 'ko')\n",
    "plt.xlabel('Years')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Numpy\n",
    "\n",
    "Numpy has a simple linear interpolation module ([np.interp](https://numpy.org/doc/stable/reference/generated/numpy.interp.html)).  It takes the independent variable (which we might call the x-coordinate or the x-axis, which for us will frequently be the 'time' dimension of some sort) and the x- and y-axis values for which there are data, and returns interpolated values for all the x-axis locations.  The x-axis coordinate (as well as those x-axis coordinates with valid data) is expected to be increasing.  Here's an example, applied to our simulated missing data series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index of the values of stm that are NOT NaNs\n",
    "valid = ~np.isnan(stm)\n",
    "\n",
    "# interpolation\n",
    "stm_linear = np.interp(t, t[valid], stm[valid])\n",
    "\n",
    "# Plot original, original with missing, and interpolation\n",
    "plt.figure()\n",
    "plt.plot(t, st, 'r-')\n",
    "plt.plot(t, stm, 'ko')\n",
    "plt.plot(t, stm_linear, 'b--')\n",
    "plt.xlabel('Years')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear interpolation (however applied) does OK here during linear parts of the function, but can't reproduce the curved crest of the sine wave, not surprisingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Scipy\n",
    "\n",
    "Scipy has a [much more extensive set of interpolation tools](https://docs.scipy.org/doc/scipy/reference/interpolate.html) -- indeed, an entire sub-package of them.  These include a wide variety of different functions that can be applied to missing data estimation.  A commonly used function is [interp1d](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html#scipy.interpolate.interp1d), although note that this is now a legacy function whose operations are now meant to be taken up by different more specific functions.  `interp1d` gives you many options for the kind of interpolation you can do - ‘linear’, ‘nearest’, ‘nearest-up’, ‘zero’, ‘slinear’, ‘quadratic’, ‘cubic’, ‘previous’, or ‘next’. The options ‘zero’, ‘slinear’, ‘quadratic’ and ‘cubic’ all refer to [splines](https://en.wikipedia.org/wiki/Spline_interpolation) of increasing orders (0th order to 3rd order).  \n",
    "\n",
    "As with other sub-packages in Scipy, interpolation actually takes two steps.  First, you construt the interpolator (here using `interp1d`) and then apply the interpolator you created to the x-coordinate.  `interp1d` also accepts a input option for `fill_value`, which tells Scipy what to do for values outside the data range (e.g. at the end of series, for instance).  \n",
    "\n",
    "Let's look at a few examples using `interp1d` specifically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# index to non NaN values in the y-coordinate\n",
    "valid = ~np.isnan(stm)\n",
    "\n",
    "# create the interpolator, specifying a cubic spline and extrapolation if necessary \n",
    "cubic_interpolator = interp1d(t[valid], stm[valid], kind='cubic', fill_value=\"extrapolate\") # Builds the interpolator (can use 'linear', 'nearest', 'cubic', 'quadratic', and others) \n",
    "\n",
    "# call the interpolator by passing it all of the x-coordinate \n",
    "stm_cubic = cubic_interpolator(t) # give the time vector to the interpolator\n",
    "\n",
    "# Plot original, original with missing, and interpolation\n",
    "plt.figure()\n",
    "plt.plot(t, st, 'r-')\n",
    "plt.plot(t, stm, 'ko')\n",
    "plt.plot(t, stm_cubic, 'b--')\n",
    "plt.xlabel('Years')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the cubic spline is a good choice for our particular time series and the pattern of missingness, as it can follow the curve of the sine wave crest.  We can see if a different flexible curve does as well, applying now the 'quadratic' method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the interpolator, specifying a cubic spline and extrapolation if necessary \n",
    "quadratic_interpolator = interp1d(t[valid], stm[valid], kind='quadratic', fill_value=\"extrapolate\") # Builds the interpolator (can use 'linear', 'nearest', 'cubic', 'quadratic', and others) \n",
    "\n",
    "# call the interpolator by passing it all of the x-coordinate \n",
    "stm_quadratic = quadratic_interpolator(t) # give the time vector to the interpolator\n",
    "\n",
    "# Plot original, original with missing, and interpolation\n",
    "plt.figure()\n",
    "plt.plot(t, st, 'r-')\n",
    "plt.plot(t, stm, 'ko')\n",
    "plt.plot(t, stm_quadratic, 'b--')\n",
    "plt.xlabel('Years')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic spline also does a good job. \n",
    "\n",
    "As `interp1d` is now deprecated, we should also look at the newer way of calling different interpolation functions by importing the specific interpolator classes.  Here we'll see the way to import the `pchip` (Piecewise Cubic Hermite Interpolating Polynomial) class and apply it to our missing data series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import PchipInterpolator\n",
    "\n",
    "pchip_interpolator = PchipInterpolator(t[valid], stm[valid])\n",
    "\n",
    "stm_pchip = pchip_interpolator(t)\n",
    "\n",
    "# Plot original, original with missing, and interpolation\n",
    "plt.figure()\n",
    "plt.plot(t, st, 'r-')\n",
    "plt.plot(t, stm, 'ko')\n",
    "plt.plot(t, stm_pchip, 'b--')\n",
    "plt.xlabel('Years')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the PCHIP approach is not as good as the spline here, as it fails to reproduce the crest of the waveform from the original data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolating in Pandas DataFrames or DataSeries\n",
    "\n",
    "Pandas contains its own [interpolation function](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html), but behind the scenes is [using Scipy](https://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation).  For DataFrames, you can decide which dimension to apply the interpolation (row-wise or column-wise), whereas for a DataSeries the interpolation is just on the available dimension. The interpolation function in Pandas accepts very similar methods to Scipy, not surprisingly, but is also capable of understanding the index as a time variable and acting accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy array to a pandas Series \n",
    "stm_series = pd.Series(stm)\n",
    "\n",
    "# call the Nearest neighbor interpolation directly on the DataSeries\n",
    "stm_nearest_pd = stm_series.interpolate(method='nearest') # others to try: ‘barycentric’, ‘polynomial’ \n",
    "\n",
    "# Plot original, original with missing, and interpolation\n",
    "plt.figure()\n",
    "plt.plot(t, st, 'r-')\n",
    "plt.plot(t, stm, 'ko')\n",
    "plt.plot(t, stm_nearest_pd, 'b--')\n",
    "\n",
    "plt.xlabel('Years')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest neighbor type approaches are probably reasonable for missing-at-random points, but for multiple missing values in a strongly structured function like the sine wave, the results are not as good as a spline fits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation in scikit-learn\n",
    "\n",
    "scikit-learn contains [several approaches to imputation](https://scikit-learn.org/1.5/modules/impute.html) - or replacing missing values with some other (hopefully reasonable values). The [`SimpleImputer`](https://scikit-learn.org/1.5/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer) largely replaces missing values with things like the mean, median, a constant, or most frequent value.  These are reasonable choices for some patterns of missing data, depending on the underlying structure of the data and how randomly the data are missing and what the ultimate purpose of the missing value replacement is for your inference.  Other datasets might require something more sophistocated.  The [`IterativeImputer`](https://scikit-learn.org/1.5/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer) allows you to specify a wide range of estimators from scikit-learn.  Here's just one example, using a Random Forest regression approach on our missing data series from above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.gaussian_process.kernels import RBF, ExpSineSquared\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# combine our data into a single variable\n",
    "data_combined = np.column_stack((t, stm.reshape(-1, 1)))  \n",
    "\n",
    "# set the estimator\n",
    "estimator = RandomForestRegressor(warm_start=True)\n",
    "\n",
    "# Initialize the IterativeImputer.\n",
    "imputer = IterativeImputer(estimator=estimator,max_iter=100)\n",
    "\n",
    "# Fit the imputer on the data and transform it to fill in missing values.\n",
    "stm_imputed = imputer.fit_transform(data_combined)\n",
    "\n",
    "# Plot original, original with missing, and interpolation\n",
    "plt.figure()\n",
    "plt.plot(t, st, 'r-')\n",
    "plt.plot(t, stm, 'ko')\n",
    "plt.plot(stm_imputed[:,0], stm_imputed[:,1], 'b--')\n",
    "\n",
    "plt.xlabel('Years')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of potential to use scikit-learn for sophisticated imputation approaches - see here: https://scikit-learn.org/1.5/modules/impute.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampling is related to interpolation in that we seek to put our y-values on a different set of x-coordinates, usually based on interpolation as we did above for missing values.  We could understand the new set of x-coordinates as including 'missing' locations on the x-axis of our series or function.  A frequent use of resampling is to place a time series on a different time base - perhaps higher or lower resolution (upscaling or downscaling) or making an irregularly observed process into one with a consistent time step (for the paleoclimatologists in the room, especially those using time-uncertain records, you might do this before using spectral analysis, for instance).\n",
    "\n",
    "Here's I'll demonstrate this on the [~800,000 CO2 record from Antarctic ice cores].   The full citation for these data is:\n",
    "\n",
    "> Bernhard Bereiter, Sarah Eggleston, Jochen Schmitt, Christoph Nehrbass-Ahles, Thomas F. Stocker, Hubertus Fischer, Sepp Kipfstuhl and Jerome Chappellaz. 2015. Revision of the EPICA Dome C CO2 record from 800 to 600 kyr before present. *Geophysical Research Letters*, doi: 10.1002/2014GL061957 \n",
    "\n",
    "We'll use Pandas to read in the data, extract the temporal data (which is in calibrated years Before Present (BP), which actually means years before 1950), make some diagnostic plots, and then interpolate the age model to a regular time step of 100 years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the ice core time series\n",
    "df = pd.read_csv('co2composite.csv') # this line use Pandas to read in the CSV file.  You might need to change the path to the file\n",
    "\n",
    "# let's extract the columns we want\n",
    "co2 = np.asarray(df[\"co2_ppm\"])\n",
    "ice_age = df[\"age_gas_calBP\"] # this is in calibrated years BP, which means years before 1950\n",
    "\n",
    "# let's plot the data\n",
    "plt.plot(1950-ice_age,co2)\n",
    "plt.xticks(np.linspace(-800000,0,5));\n",
    "plt.xlabel('YEAR BP');\n",
    "plt.ylabel('$CO_{2}$ (ppm)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a histogram, we can see the range of different time-steps between the CO2 data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a histogram of the difference between each age\n",
    "plt.hist(np.diff(ice_age));\n",
    "plt.xlabel('Gap between $CO_2$ samples (years)');\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time steps range from <100 years to many thousands of years.  It would be a challenge to do the type of spectral analysis we've done in class on these data.  But we can use `interp1d` again to place these CO2 data on a regular time step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this next call to interpolate creates the function (f) which will do the interpolation for us\n",
    "resampler = interp1d(ice_age, co2) # we'll use f to put CO2 on a new age scale\n",
    "age_new = np.arange(-51, 800000, 100) # let's put the ice core on 100 year spacing\n",
    "co2_new = resampler(age_new) # using the interpolate function f we created before, apply the new ages to the CO2 series\n",
    "\n",
    "# ... and let's plot to make sure everything looks OK\n",
    "plt.plot(1950-age_new,co2_new) # interpolated data\n",
    "plt.plot(1950-ice_age,co2,'k.',markersize=3) # original data\n",
    "plt.xticks(np.linspace(-800000,0,5));\n",
    "plt.xlabel('YEAR BP');\n",
    "plt.ylabel('$CO_{2}$ (ppm)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that all the time steps are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a histogram of the difference between each age\n",
    "plt.hist(np.diff(age_new));\n",
    "plt.xlabel('Gap between $CO_2$ samples (years)');\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Try to create a more complex curve (e.g. the sum of different sine and cosines) with missing values and see how different interpolation methods do at a more challenging target\n",
    "2. Try and create white or red noise curve without the regular structure of a sine or cosine - which methods seem to work best in this case? \n",
    "3. Perform MTM spectral analysis on the resampled CO2 data, above.  Try different interpolations (e.g. non-linear, different time steps in the new age model) - can you detect any consequences of the resampling choices in the spectrum for this particular time series?  How generalizable do you think this observation might be? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geog696fpython311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
